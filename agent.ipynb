{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "import yaml\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import command_generation_memory\n",
        "import qa_memory\n",
        "from model import DQN\n",
        "from layers import compute_mask, NegativeLogLoss\n",
        "from generic import to_np, to_pt, preproc, _words_to_ids, pad_sequences\n",
        "from generic import max_len, ez_gather_dim_1, ObservationPool\n",
        "from generic import list_of_token_list_to_char_input\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.mode = \"train\"\n",
        "        with open(\"config.yaml\") as reader:\n",
        "            self.config = yaml.safe_load(reader)\n",
        "        print(self.config)\n",
        "        self.load_config()\n",
        "\n",
        "        self.online_net = DQN(config=self.config,\n",
        "                              word_vocab=self.word_vocab,\n",
        "                              char_vocab=self.char_vocab,\n",
        "                              answer_type=self.answer_type)\n",
        "        self.target_net = DQN(config=self.config,\n",
        "                              word_vocab=self.word_vocab,\n",
        "                              char_vocab=self.char_vocab,\n",
        "                              answer_type=self.answer_type)\n",
        "        self.online_net.train()\n",
        "        self.target_net.train()\n",
        "        self.update_target_net()\n",
        "        for param in self.target_net.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.online_net.cuda()\n",
        "            self.target_net.cuda()\n",
        "\n",
        "        self.naozi = ObservationPool(capacity=self.naozi_capacity)\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=self.config['training']['optimizer']['learning_rate'])\n",
        "        self.clip_grad_norm = self.config['training']['optimizer']['clip_grad_norm']\n",
        "\n",
        "    def load_config(self):\n",
        "        # word vocab\n",
        "        with open(\"vocabularies/word_vocab.txt\") as f:\n",
        "            self.word_vocab = f.read().split(\"\\n\")\n",
        "        self.word2id = {}\n",
        "        for i, w in enumerate(self.word_vocab):\n",
        "            self.word2id[w] = i\n",
        "        # char vocab\n",
        "        with open(\"vocabularies/char_vocab.txt\") as f:\n",
        "            self.char_vocab = f.read().split(\"\\n\")\n",
        "        self.char2id = {}\n",
        "        for i, w in enumerate(self.char_vocab):\n",
        "            self.char2id[w] = i\n",
        "\n",
        "        self.EOS_id = self.word2id[\"</s>\"]\n",
        "        self.train_data_size = self.config['general']['train_data_size']\n",
        "        self.question_type = self.config['general']['question_type']\n",
        "        self.random_map = self.config['general']['random_map']\n",
        "        self.testset_path =  self.config['general']['testset_path']\n",
        "        self.naozi_capacity = self.config['general']['naozi_capacity']\n",
        "        self.eval_folder = pjoin(self.testset_path, self.question_type, (\"random_map\" if self.random_map else \"fixed_map\"))\n",
        "        self.eval_data_path = pjoin(self.testset_path, \"data.json\")\n",
        "\n",
        "        self.batch_size = self.config['training']['batch_size']\n",
        "        self.max_nb_steps_per_episode = self.config['training']['max_nb_steps_per_episode']\n",
        "        self.max_episode = self.config['training']['max_episode']\n",
        "        self.target_net_update_frequency = self.config['training']['target_net_update_frequency']\n",
        "        self.learn_start_from_this_episode = self.config['training']['learn_start_from_this_episode']\n",
        "        \n",
        "        self.run_eval = self.config['evaluate']['run_eval']\n",
        "        self.eval_batch_size = self.config['evaluate']['batch_size']\n",
        "        self.eval_max_nb_steps_per_episode = self.config['evaluate']['max_nb_steps_per_episode']\n",
        "\n",
        "        # Set the random seed manually for reproducibility.\n",
        "        self.random_seed = self.config['general']['random_seed']\n",
        "        np.random.seed(self.random_seed)\n",
        "        torch.manual_seed(self.random_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            if not self.config['general']['use_cuda']:\n",
        "                print(\"WARNING: CUDA device detected but 'use_cuda: false' found in config.yaml\")\n",
        "                self.use_cuda = False\n",
        "            else:\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.cuda.manual_seed(self.random_seed)\n",
        "                self.use_cuda = True\n",
        "        else:\n",
        "            self.use_cuda = False\n",
        "\n",
        "        if self.question_type == \"location\":\n",
        "            self.answer_type = \"pointing\"\n",
        "        elif self.question_type in [\"attribute\", \"existence\"]:\n",
        "            self.answer_type = \"2 way\"\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.save_checkpoint = self.config['checkpoint']['save_checkpoint']\n",
        "        self.experiment_tag = self.config['checkpoint']['experiment_tag']\n",
        "        self.save_frequency = self.config['checkpoint']['save_frequency']\n",
        "        self.load_pretrained = self.config['checkpoint']['load_pretrained']\n",
        "        self.load_from_tag = self.config['checkpoint']['load_from_tag']\n",
        "\n",
        "        self.qa_loss_lambda = self.config['training']['qa_loss_lambda']\n",
        "        self.interaction_loss_lambda = self.config['training']['interaction_loss_lambda']\n",
        "\n",
        "        # replay buffer and updates\n",
        "        self.discount_gamma = self.config['replay']['discount_gamma']\n",
        "        self.replay_batch_size = self.config['replay']['replay_batch_size']\n",
        "        self.command_generation_replay_memory = command_generation_memory.PrioritizedReplayMemory(self.config['replay']['replay_memory_capacity'],\n",
        "                                                                                                  priority_fraction=self.config['replay']['replay_memory_priority_fraction'],\n",
        "                                                                                                  discount_gamma=self.discount_gamma)\n",
        "        self.qa_replay_memory = qa_memory.PrioritizedReplayMemory(self.config['replay']['replay_memory_capacity'],\n",
        "                                                                  priority_fraction=0.0)\n",
        "        self.update_per_k_game_steps = self.config['replay']['update_per_k_game_steps']\n",
        "        self.multi_step = self.config['replay']['multi_step']\n",
        "\n",
        "        # distributional RL\n",
        "        self.use_distributional = self.config['distributional']['enable']\n",
        "        self.atoms = self.config['distributional']['atoms']\n",
        "        self.v_min = self.config['distributional']['v_min']\n",
        "        self.v_max = self.config['distributional']['v_max']\n",
        "        self.support = torch.linspace(self.v_min, self.v_max, self.atoms)  # Support (range) of z\n",
        "        if self.use_cuda:\n",
        "            self.support = self.support.cuda()\n",
        "        self.delta_z = (self.v_max - self.v_min) / (self.atoms - 1)\n",
        "\n",
        "        # dueling networks\n",
        "        self.dueling_networks = self.config['dueling_networks']\n",
        "\n",
        "        # double dqn\n",
        "        self.double_dqn = self.config['double_dqn']\n",
        "\n",
        "        # counting reward\n",
        "        self.revisit_counting_lambda_anneal_episodes = self.config['episodic_counting_bonus']['revisit_counting_lambda_anneal_episodes']\n",
        "        self.revisit_counting_lambda_anneal_from = self.config['episodic_counting_bonus']['revisit_counting_lambda_anneal_from']\n",
        "        self.revisit_counting_lambda_anneal_to = self.config['episodic_counting_bonus']['revisit_counting_lambda_anneal_to']\n",
        "        self.revisit_counting_lambda = self.revisit_counting_lambda_anneal_from\n",
        "\n",
        "        # valid command bonus\n",
        "        self.valid_command_bonus_lambda = self.config['valid_command_bonus_lambda']\n",
        "\n",
        "        # epsilon greedy\n",
        "        self.epsilon_anneal_episodes = self.config['epsilon_greedy']['epsilon_anneal_episodes']\n",
        "        self.epsilon_anneal_from = self.config['epsilon_greedy']['epsilon_anneal_from']\n",
        "        self.epsilon_anneal_to = self.config['epsilon_greedy']['epsilon_anneal_to']\n",
        "        self.epsilon = self.epsilon_anneal_from\n",
        "        self.noisy_net = self.config['epsilon_greedy']['noisy_net']\n",
        "        if self.noisy_net:\n",
        "            # disable epsilon greedy\n",
        "            self.epsilon_anneal_episodes = -1\n",
        "            self.epsilon = 0.0\n",
        "\n",
        "        self.nlp = spacy.load('en', disable=['ner', 'parser', 'tagger'])\n",
        "        self.single_word_verbs = set([\"inventory\", \"look\", \"wait\"])\n",
        "        self.two_word_verbs = set([\"go\"])\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Tell the agent that it's training phase.\n",
        "        \"\"\"\n",
        "        self.mode = \"train\"\n",
        "        self.online_net.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Tell the agent that it's evaluation phase.\n",
        "        \"\"\"\n",
        "        self.mode = \"eval\"\n",
        "        self.online_net.eval()\n",
        "\n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
        "\n",
        "    def reset_noise(self):\n",
        "        if self.noisy_net:\n",
        "            # Resets noisy weights in all linear layers (of online net only)\n",
        "            self.online_net.reset_noise()\n",
        "            \n",
        "    def zero_noise(self):\n",
        "        if self.noisy_net:\n",
        "            self.online_net.zero_noise()\n",
        "            self.target_net.zero_noise()\n",
        "\n",
        "    def load_pretrained_model(self, load_from):\n",
        "        \"\"\"\n",
        "        Load pretrained checkpoint from file.\n",
        "\n",
        "        Arguments:\n",
        "            load_from: File name of the pretrained model checkpoint.\n",
        "        \"\"\"\n",
        "        print(\"loading model from %s\\n\" % (load_from))\n",
        "        try:\n",
        "            if self.use_cuda:\n",
        "                state_dict = torch.load(load_from)\n",
        "            else:\n",
        "                state_dict = torch.load(load_from, map_location='cpu')\n",
        "            self.online_net.load_state_dict(state_dict)\n",
        "        except:\n",
        "            print(\"Failed to load checkpoint...\")\n",
        "\n",
        "    def save_model_to_path(self, save_to):\n",
        "        torch.save(self.online_net.state_dict(), save_to)\n",
        "        print(\"Saved checkpoint to %s...\" % (save_to))\n",
        "\n",
        "    def init(self, obs, infos):\n",
        "        \"\"\"\n",
        "        Prepare the agent for the upcoming games.\n",
        "\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        # reset agent, get vocabulary masks for verbs / adjectives / nouns\n",
        "        batch_size = len(obs)\n",
        "        self.reset_binarized_counter(batch_size)\n",
        "        self.not_finished_yet = np.ones((batch_size,), dtype=\"float32\")\n",
        "        self.prev_actions = [[\"\" for _ in range(batch_size)]]\n",
        "        self.prev_step_is_still_interacting = np.ones((batch_size,), dtype=\"float32\")  # 1s and starts to be 0 when previous action is \"wait\"\n",
        "        self.naozi.reset(batch_size=batch_size)\n",
        "\n",
        "    def get_agent_inputs(self, string_list):\n",
        "        sentence_token_list = [item.split() for item in string_list]\n",
        "        sentence_id_list = [_words_to_ids(tokens, self.word2id) for tokens in sentence_token_list]\n",
        "        input_sentence_char = list_of_token_list_to_char_input(sentence_token_list, self.char2id)\n",
        "        input_sentence = pad_sequences(sentence_id_list, maxlen=max_len(sentence_id_list)).astype('int32')\n",
        "        input_sentence = to_pt(input_sentence, self.use_cuda)\n",
        "        input_sentence_char = to_pt(input_sentence_char, self.use_cuda)\n",
        "        return input_sentence, input_sentence_char, sentence_id_list\n",
        "\n",
        "    def get_game_info_at_certain_step(self, obs, infos):\n",
        "        \"\"\"\n",
        "        Get all needed info from game engine for training.\n",
        "        Arguments:\n",
        "            obs: Previous command's feedback for each game.\n",
        "            infos: Additional information for each game.\n",
        "        \"\"\"\n",
        "        batch_size = len(obs)\n",
        "        feedback_strings = [preproc(item, tokenizer=self.nlp) for item in obs]\n",
        "        description_strings = [preproc(item, tokenizer=self.nlp) for item in infos[\"description\"]]\n",
        "        observation_strings = [d + \" <|> \" + fb if fb != d else d + \" <|> hello\" for fb, d in zip(feedback_strings, description_strings)]\n",
        "\n",
        "        inventory_strings = [preproc(item, tokenizer=self.nlp) for item in infos[\"inventory\"]]\n",
        "        local_word_list = [obs.split() + inv.split() for obs, inv in zip(observation_strings, inventory_strings)]\n",
        "\n",
        "        directions = [\"east\", \"west\", \"north\", \"south\"]\n",
        "        if self.question_type in [\"location\", \"existence\"]:\n",
        "            # agents observes the env, but do not change them\n",
        "            possible_verbs = [[\"go\", \"inventory\", \"wait\", \"open\", \"examine\"] for _ in range(batch_size)]\n",
        "        else:\n",
        "            possible_verbs = [list(set(item) - set([\"\", \"look\"])) for item in infos[\"verbs\"]]\n",
        "\n",
        "        possible_adjs, possible_nouns = [], []\n",
        "        for i in range(batch_size):\n",
        "            object_nouns = [item.split()[-1] for item in infos[\"object_nouns\"][i]]\n",
        "            object_adjs = [w for item in infos[\"object_adjs\"][i] for w in item.split()]\n",
        "            possible_nouns.append(list(set(object_nouns) & set(local_word_list[i]) - set([\"\"])) + directions)\n",
        "            possible_adjs.append(list(set(object_adjs) & set(local_word_list[i]) - set([\"\"])) + [\"</s>\"])\n",
        "\n",
        "        return observation_strings, [possible_verbs, possible_adjs, possible_nouns]\n",
        "\n",
        "    def get_state_strings(self, infos):\n",
        "        description_strings = infos[\"description\"]\n",
        "        inventory_strings = infos[\"inventory\"]\n",
        "        observation_strings = [_d + _i for (_d, _i) in zip(description_strings, inventory_strings)]\n",
        "        return observation_strings\n",
        "\n",
        "    def get_local_word_masks(self, possible_words):\n",
        "        possible_verbs, possible_adjs, possible_nouns = possible_words\n",
        "        batch_size = len(possible_verbs)\n",
        "\n",
        "        verb_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        noun_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        adj_mask = np.zeros((batch_size, len(self.word_vocab)), dtype=\"float32\")\n",
        "        for i in range(batch_size):\n",
        "            for w in possible_verbs[i]:\n",
        "                if w in self.word2id:\n",
        "                    verb_mask[i][self.word2id[w]] = 1.0\n",
        "            for w in possible_adjs[i]:\n",
        "                if w in self.word2id:\n",
        "                    adj_mask[i][self.word2id[w]] = 1.0\n",
        "            for w in possible_nouns[i]:\n",
        "                if w in self.word2id:\n",
        "                    noun_mask[i][self.word2id[w]] = 1.0\n",
        "        adj_mask[:, self.EOS_id] = 1.0\n",
        "\n",
        "        return [verb_mask, adj_mask, noun_mask]\n",
        "\n",
        "    def get_match_representations(self, input_observation, input_observation_char, input_quest, input_quest_char, use_model=\"online\"):\n",
        "        model = self.online_net if use_model == \"online\" else self.target_net\n",
        "        description_representation_sequence, description_mask = model.representation_generator(input_observation, input_observation_char)\n",
        "        quest_representation_sequence, quest_mask = model.representation_generator(input_quest, input_quest_char)\n",
        "\n",
        "        match_representation_sequence = model.get_match_representations(description_representation_sequence,\n",
        "                                                                        description_mask,\n",
        "                                                                        quest_representation_sequence,\n",
        "                                                                        quest_mask)\n",
        "        match_representation_sequence = match_representation_sequence * description_mask.unsqueeze(-1)\n",
        "        return match_representation_sequence\n",
        "\n",
        "    def get_ranks(self, input_observation, input_observation_char, input_quest, input_quest_char, word_masks, use_model=\"online\"):\n",
        "        \"\"\"\n",
        "        Given input observation and question tensors, to get Q values of words.\n",
        "        \"\"\"\n",
        "        model = self.online_net if use_model == \"online\" else self.target_net\n",
        "        match_representation_sequence = self.get_match_representations(input_observation, input_observation_char, input_quest, input_quest_char, use_model=use_model)\n",
        "        action_ranks = model.action_scorer(match_representation_sequence, word_masks)  # list of 3 tensors\n",
        "        return action_ranks\n",
        "\n",
        "    def choose_maxQ_command(self, action_ranks, word_mask=None):\n",
        "        \"\"\"\n",
        "        Generate a command by maximum q values, for epsilon greedy.\n",
        "        \"\"\"\n",
        "        if self.use_distributional:\n",
        "            action_ranks = [(item * self.support).sum(2) for item in action_ranks]  # list of batch x n_vocab\n",
        "        action_indices = []\n",
        "        for i in range(len(action_ranks)):\n",
        "            ar = action_ranks[i]\n",
        "            ar = ar - torch.min(ar, -1, keepdim=True)[0] + 1e-2  # minus the min value, so that all values are non-negative\n",
        "            if word_mask is not None:\n",
        "                assert word_mask[i].size() == ar.size(), (word_mask[i].size().shape, ar.size())\n",
        "                ar = ar * word_mask[i]\n",
        "            action_indices.append(torch.argmax(ar, -1))  # batch\n",
        "        return action_indices\n",
        "\n",
        "    def choose_random_command(self, batch_size, action_space_size, possible_words=None):\n",
        "        \"\"\"\n",
        "        Generate a command randomly, for epsilon greedy.\n",
        "        \"\"\"\n",
        "        action_indices = []\n",
        "        for i in range(3):\n",
        "            if possible_words is None:\n",
        "                indices = np.random.choice(action_space_size, batch_size)\n",
        "            else:\n",
        "                indices = []\n",
        "                for j in range(batch_size):                \n",
        "                    mask_ids = []\n",
        "                    for w in possible_words[i][j]:\n",
        "                        if w in self.word2id:\n",
        "                            mask_ids.append(self.word2id[w])\n",
        "                    indices.append(np.random.choice(mask_ids))\n",
        "                indices = np.array(indices)\n",
        "            action_indices.append(to_pt(indices, self.use_cuda))  # batch\n",
        "        return action_indices\n",
        "\n",
        "    def get_chosen_strings(self, chosen_indices):\n",
        "        \"\"\"\n",
        "        Turns list of word indices into actual command strings.\n",
        "        chosen_indices: Word indices chosen by model.\n",
        "        \"\"\"\n",
        "        chosen_indices_np = [to_np(item) for item in chosen_indices]\n",
        "        res_str = []\n",
        "        batch_size = chosen_indices_np[0].shape[0]\n",
        "        for i in range(batch_size):\n",
        "            verb, adj, noun = chosen_indices_np[0][i], chosen_indices_np[1][i], chosen_indices_np[2][i]\n",
        "            res_str.append(self.word_ids_to_commands(verb, adj, noun))\n",
        "        return res_str\n",
        "\n",
        "    def word_ids_to_commands(self, verb, adj, noun):\n",
        "        \"\"\"\n",
        "        Turn the 3 indices into actual command strings.\n",
        "\n",
        "        Arguments:\n",
        "            verb: Index of the guessing verb in vocabulary\n",
        "            adj: Index of the guessing adjective in vocabulary\n",
        "            noun: Index of the guessing noun in vocabulary\n",
        "        \"\"\"\n",
        "        # turns 3 indices into actual command strings\n",
        "        if self.word_vocab[verb] in self.single_word_verbs:\n",
        "            return self.word_vocab[verb]\n",
        "        if self.word_vocab[verb] in self.two_word_verbs:\n",
        "            return \" \".join([self.word_vocab[verb], self.word_vocab[noun]])\n",
        "        if adj == self.EOS_id:\n",
        "            return \" \".join([self.word_vocab[verb], self.word_vocab[noun]])\n",
        "        else:\n",
        "            return \" \".join([self.word_vocab[verb], self.word_vocab[adj], self.word_vocab[noun]])\n",
        "\n",
        "    def act_random(self, obs, infos, input_observation, input_observation_char, input_quest, input_quest_char, possible_words):\n",
        "        with torch.no_grad():\n",
        "            batch_size = len(obs)\n",
        "            word_indices_random = self.choose_random_command(batch_size, len(self.word_vocab), possible_words)\n",
        "            chosen_indices = word_indices_random\n",
        "            chosen_strings = self.get_chosen_strings(chosen_indices)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if chosen_strings[i] == \"wait\":\n",
        "                    self.not_finished_yet[i] = 0.0\n",
        "\n",
        "            # info for replay memory\n",
        "            for i in range(batch_size):\n",
        "                if self.prev_actions[-1][i] == \"wait\":\n",
        "                    self.prev_step_is_still_interacting[i] = 0.0\n",
        "            # previous step is still interacting, this is because DQN requires one step extra computation\n",
        "            replay_info = [chosen_indices, to_pt(self.prev_step_is_still_interacting, self.use_cuda, \"float\")]\n",
        "\n",
        "            # cache new info in current game step into caches\n",
        "            self.prev_actions.append(chosen_strings)\n",
        "            return chosen_strings, replay_info\n",
        "\n",
        "    def act_greedy(self, obs, infos, input_observation, input_observation_char, input_quest, input_quest_char, possible_words):\n",
        "        \"\"\"\n",
        "        Acts upon the current list of observations.\n",
        "        One text command must be returned for each observation.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            batch_size = len(obs)\n",
        "            local_word_masks_np = self.get_local_word_masks(possible_words)\n",
        "            local_word_masks = [to_pt(item, self.use_cuda, type=\"float\") for item in local_word_masks_np]\n",
        "    \n",
        "            # generate commands for one game step, epsilon greedy is applied, i.e.,\n",
        "            # there is epsilon of chance to generate random commands\n",
        "            action_ranks = self.get_ranks(input_observation, input_observation_char, input_quest, input_quest_char, local_word_masks, use_model=\"online\")  # list of batch x vocab\n",
        "            word_indices_maxq = self.choose_maxQ_command(action_ranks, local_word_masks)\n",
        "            chosen_indices = word_indices_maxq\n",
        "            chosen_strings = self.get_chosen_strings(chosen_indices)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if chosen_strings[i] == \"wait\":\n",
        "                    self.not_finished_yet[i] = 0.0\n",
        "\n",
        "            # info for replay memory\n",
        "            for i in range(batch_size):\n",
        "                if self.prev_actions[-1][i] == \"wait\":\n",
        "                    self.prev_step_is_still_interacting[i] = 0.0\n",
        "            # previous step is still interacting, this is because DQN requires one step extra computation\n",
        "            replay_info = [chosen_indices, to_pt(self.prev_step_is_still_interacting, self.use_cuda, \"float\")]\n",
        "\n",
        "            # cache new info in current game step into caches\n",
        "            self.prev_actions.append(chosen_strings)\n",
        "            return chosen_strings, replay_info\n",
        "\n",
        "    def act(self, obs, infos, input_observation, input_observation_char, input_quest, input_quest_char, possible_words, random=False):\n",
        "        \"\"\"\n",
        "        Acts upon the current list of observations.\n",
        "        One text command must be returned for each observation.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            if self.mode == \"eval\":\n",
        "                return self.act_greedy(obs, infos, input_observation, input_observation_char, input_quest, input_quest_char, possible_words)\n",
        "            if random:\n",
        "                return self.act_random(obs, infos, input_observation, input_observation_char, input_quest, input_quest_char, possible_words)\n",
        "            batch_size = len(obs)\n",
        "\n",
        "            local_word_masks_np = self.get_local_word_masks(possible_words)\n",
        "            local_word_masks = [to_pt(item, self.use_cuda, type=\"float\") for item in local_word_masks_np]\n",
        "    \n",
        "            # generate commands for one game step, epsilon greedy is applied, i.e.,\n",
        "            # there is epsilon of chance to generate random commands\n",
        "            action_ranks = self.get_ranks(input_observation, input_observation_char, input_quest, input_quest_char, local_word_masks, use_model=\"online\")  # list of batch x vocab\n",
        "            word_indices_maxq = self.choose_maxQ_command(action_ranks, local_word_masks)\n",
        "            word_indices_random = self.choose_random_command(batch_size, len(self.word_vocab), possible_words)\n",
        "    \n",
        "            # random number for epsilon greedy\n",
        "            rand_num = np.random.uniform(low=0.0, high=1.0, size=(batch_size,))\n",
        "            less_than_epsilon = (rand_num < self.epsilon).astype(\"float32\")  # batch\n",
        "            greater_than_epsilon = 1.0 - less_than_epsilon\n",
        "            less_than_epsilon = to_pt(less_than_epsilon, self.use_cuda, type='long')\n",
        "            greater_than_epsilon = to_pt(greater_than_epsilon, self.use_cuda, type='long')\n",
        "            chosen_indices = [less_than_epsilon * idx_random + greater_than_epsilon * idx_maxq for idx_random, idx_maxq in zip(word_indices_random, word_indices_maxq)]\n",
        "            chosen_strings = self.get_chosen_strings(chosen_indices)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if chosen_strings[i] == \"wait\":\n",
        "                    self.not_finished_yet[i] = 0.0\n",
        "\n",
        "            # info for replay memory\n",
        "            for i in range(batch_size):\n",
        "                if self.prev_actions[-1][i] == \"wait\":\n",
        "                    self.prev_step_is_still_interacting[i] = 0.0\n",
        "            # previous step is still interacting, this is because DQN requires one step extra computation\n",
        "            replay_info = [chosen_indices, to_pt(self.prev_step_is_still_interacting, self.use_cuda, \"float\")]\n",
        "\n",
        "            # cache new info in current game step into caches\n",
        "            self.prev_actions.append(chosen_strings)\n",
        "            return chosen_strings, replay_info\n",
        "\n",
        "    def get_dqn_loss(self):\n",
        "        \"\"\"\n",
        "        Update neural model in agent. In this example we follow algorithm\n",
        "        of updating model in dqn with replay memory.\n",
        "        \"\"\"\n",
        "        if len(self.command_generation_replay_memory) < self.replay_batch_size:\n",
        "            return None\n",
        "\n",
        "        data = self.command_generation_replay_memory.get_batch(self.replay_batch_size, self.multi_step)\n",
        "        if data is None:\n",
        "            return None\n",
        "\n",
        "        obs_list, quest_list, possible_words_list, chosen_indices, rewards, next_obs_list, next_possible_words_list, actual_n_list = data\n",
        "        batch_size = len(actual_n_list)\n",
        "\n",
        "        input_quest, input_quest_char, _ = self.get_agent_inputs(quest_list)\n",
        "        input_observation, input_observation_char, _ =  self.get_agent_inputs(obs_list)\n",
        "        next_input_observation, next_input_observation_char, _ =  self.get_agent_inputs(next_obs_list)\n",
        "\n",
        "        possible_words, next_possible_words = [], []\n",
        "        for i in range(3):\n",
        "            possible_words.append([item[i] for item in possible_words_list])\n",
        "            next_possible_words.append([item[i] for item in next_possible_words_list])\n",
        "\n",
        "        local_word_masks = [to_pt(item, self.use_cuda, type=\"float\") for item in self.get_local_word_masks(possible_words)]\n",
        "        next_local_word_masks = [to_pt(item, self.use_cuda, type=\"float\") for item in self.get_local_word_masks(next_possible_words)]\n",
        "\n",
        "        action_ranks = self.get_ranks(input_observation, input_observation_char, input_quest, input_quest_char, local_word_masks, use_model=\"online\")  # list of batch x vocab or list of batch x vocab x atoms\n",
        "        # ps_a\n",
        "        word_qvalues = [ez_gather_dim_1(w_rank, idx.unsqueeze(-1)).squeeze(1) for w_rank, idx in zip(action_ranks, chosen_indices)]  # list of batch or list of batch x atoms\n",
        "        q_value = torch.mean(torch.stack(word_qvalues, -1), -1)  # batch or batch x atoms\n",
        "        # log_ps_a\n",
        "        log_q_value = torch.log(q_value)  # batch or batch x atoms\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if self.noisy_net:\n",
        "                self.target_net.reset_noise()  # Sample new target net noise\n",
        "            if self.double_dqn:\n",
        "                # pns Probabilities p(s_t+n, \u00b7; \u03b8online)\n",
        "                next_action_ranks = self.get_ranks(next_input_observation, next_input_observation_char, input_quest, input_quest_char, next_local_word_masks, use_model=\"online\")  \n",
        "                # list of batch x vocab or list of batch x vocab x atoms\n",
        "                # Perform argmax action selection using online network: argmax_a[(z, p(s_t+n, a; \u03b8online))]\n",
        "                next_word_indices = self.choose_maxQ_command(next_action_ranks, next_local_word_masks)  # list of batch x 1\n",
        "                # pns # Probabilities p(s_t+n, \u00b7; \u03b8target)\n",
        "                next_action_ranks = self.get_ranks(next_input_observation, next_input_observation_char, input_quest, input_quest_char, next_local_word_masks, use_model=\"target\")  # batch x vocab or list of batch x vocab x atoms\n",
        "                # pns_a # Double-Q probabilities p(s_t+n, argmax_a[(z, p(s_t+n, a; \u03b8online))]; \u03b8target)\n",
        "                next_word_qvalues = [ez_gather_dim_1(w_rank, idx.unsqueeze(-1)).squeeze(1) for w_rank, idx in zip(next_action_ranks, next_word_indices)]  # list of batch or list of batch x atoms\n",
        "            else:\n",
        "                # pns Probabilities p(s_t+n, \u00b7; \u03b8online)\n",
        "                next_action_ranks = self.get_ranks(next_input_observation, next_input_observation_char, input_quest, input_quest_char, next_local_word_masks, use_model=\"target\")  \n",
        "                # list of batch x vocab or list of batch x vocab x atoms\n",
        "                next_word_indices = self.choose_maxQ_command(next_action_ranks, next_local_word_masks)  # list of batch x 1\n",
        "                next_word_qvalues = [ez_gather_dim_1(w_rank, idx.unsqueeze(-1)).squeeze(1) for w_rank, idx in zip(next_action_ranks, next_word_indices)]  # list of batch or list of batch x atoms\n",
        "\n",
        "            next_q_value = torch.mean(torch.stack(next_word_qvalues, -1), -1)  # batch or batch x atoms\n",
        "            # Compute Tz (Bellman operator T applied to z)\n",
        "            discount = to_pt((np.ones_like(actual_n_list) * self.discount_gamma) ** actual_n_list, self.use_cuda, type=\"float\")\n",
        "        if not self.use_distributional:\n",
        "            rewards = rewards + next_q_value * discount  # batch\n",
        "            loss = F.smooth_l1_loss(q_value, rewards)\n",
        "            return loss\n",
        "\n",
        "        with torch.no_grad():\n",
        "            Tz = rewards.unsqueeze(-1) + discount.unsqueeze(-1) * self.support.unsqueeze(0)  # Tz = R^n + (\u03b3^n)z (accounting for terminal states)\n",
        "            Tz = Tz.clamp(min=self.v_min, max=self.v_max)  # Clamp between supported values\n",
        "            # Compute L2 projection of Tz onto fixed support z\n",
        "            b = (Tz - self.v_min) / self.delta_z  # b = (Tz - Vmin) / \u0394z\n",
        "            l, u = b.floor().to(torch.int64), b.ceil().to(torch.int64)\n",
        "            # Fix disappearing probability mass when l = b = u (b is int)\n",
        "            l[(u > 0) * (l == u)] -= 1\n",
        "            u[(l < (self.atoms - 1)) * (l == u)] += 1\n",
        "\n",
        "            # Distribute probability of Tz\n",
        "            m = torch.zeros(batch_size, self.atoms).float()\n",
        "            if self.use_cuda:\n",
        "                m = m.cuda()\n",
        "            offset = torch.linspace(0, ((batch_size - 1) * self.atoms), batch_size).unsqueeze(1).expand(batch_size, self.atoms).long()\n",
        "            if self.use_cuda:\n",
        "                offset = offset.cuda()\n",
        "            m.view(-1).index_add_(0, (l + offset).view(-1), (next_q_value * (u.float() - b)).view(-1))  # m_l = m_l + p(s_t+n, a*)(u - b)\n",
        "            m.view(-1).index_add_(0, (u + offset).view(-1), (next_q_value * (b - l.float())).view(-1))  # m_u = m_u + p(s_t+n, a*)(b - l)\n",
        "\n",
        "        loss = -torch.sum(m * log_q_value, 1)  # Cross-entropy loss (minimises DKL(m||p(s_t, a_t)))\n",
        "        loss = torch.mean(loss)\n",
        "        return loss\n",
        "\n",
        "    def update_interaction(self):\n",
        "        # update neural model by replaying snapshots in replay memory\n",
        "        interaction_loss = self.get_dqn_loss()\n",
        "        if interaction_loss is None:\n",
        "            return None\n",
        "        loss = interaction_loss * self.interaction_loss_lambda\n",
        "        # Backpropagate\n",
        "        self.online_net.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), self.clip_grad_norm)\n",
        "        self.optimizer.step()  # apply gradients\n",
        "        return to_np(torch.mean(interaction_loss))\n",
        "\n",
        "    def answer_question(self, input_observation, input_observation_char, observation_id_list, input_quest, input_quest_char, use_model=\"online\"):\n",
        "        # first pad answerer_input, and get the mask\n",
        "        model = self.online_net if use_model == \"online\" else self.target_net\n",
        "        batch_size = len(observation_id_list)\n",
        "        max_length = input_observation.size(1)\n",
        "        mask = compute_mask(input_observation)  # batch x obs_len\n",
        "\n",
        "        # noun mask for location question\n",
        "        if self.question_type in [\"location\"]:\n",
        "            location_mask = []\n",
        "            for i in range(batch_size):\n",
        "                m = [1 for item in observation_id_list[i]]\n",
        "                location_mask.append(m)\n",
        "            location_mask = pad_sequences(location_mask, maxlen=max_length, dtype=\"float32\")\n",
        "            location_mask = to_pt(location_mask, enable_cuda=self.use_cuda, type='float')\n",
        "            assert mask.size() == location_mask.size()\n",
        "            mask = mask * location_mask\n",
        "\n",
        "        match_representation_sequence = self.get_match_representations(input_observation, input_observation_char, input_quest, input_quest_char, use_model=use_model)\n",
        "        pred = model.answer_question(match_representation_sequence, mask)  # batch x vocab or batch x 2\n",
        "\n",
        "        # attention sum:\n",
        "        # sometimes certain word appears multiple times in the observation,\n",
        "        # thus we need to merge them together before doing further computations\n",
        "        # ------- but\n",
        "        # if answer type is not pointing, we just use a pre-defined mapping\n",
        "        # that maps 0/1 to their positions in vocab\n",
        "        if self.answer_type == \"2 way\":\n",
        "            observation_id_list = []\n",
        "            max_length = 2\n",
        "            for i in range(batch_size):\n",
        "                observation_id_list.append([self.word2id[\"0\"], self.word2id[\"1\"]])\n",
        "\n",
        "        observation = to_pt(pad_sequences(observation_id_list, maxlen=max_length).astype('int32'), self.use_cuda)\n",
        "        vocab_distribution = np.zeros((batch_size, len(self.word_vocab)))  # batch x vocab\n",
        "        vocab_distribution = to_pt(vocab_distribution, self.use_cuda, type='float')\n",
        "        vocab_distribution = vocab_distribution.scatter_add_(1, observation, pred)  # batch x vocab\n",
        "        non_zero_words = []\n",
        "        for i in range(batch_size):\n",
        "            non_zero_words.append(list(set(observation_id_list[i])))\n",
        "        vocab_mask = torch.ne(vocab_distribution, 0).float()\n",
        "        return vocab_distribution, non_zero_words, vocab_mask\n",
        "\n",
        "    def point_maxq_position(self, vocab_distribution, mask):\n",
        "        \"\"\"\n",
        "        Generate a command by maximum q values, for epsilon greedy.\n",
        "\n",
        "        Arguments:\n",
        "            point_distribution: Q values for each position (mapped to vocab).\n",
        "            mask: vocab masks.\n",
        "        \"\"\"\n",
        "        vocab_distribution = vocab_distribution - torch.min(vocab_distribution, -1, keepdim=True)[0] + 1e-2  # minus the min value, so that all values are non-negative\n",
        "        vocab_distribution = vocab_distribution * mask  # batch x vocab\n",
        "        indices = torch.argmax(vocab_distribution, -1)  # batch\n",
        "        return indices\n",
        "\n",
        "    def answer_question_act_greedy(self, input_observation, input_observation_char, observation_id_list, input_quest, input_quest_char):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vocab_distribution, _, vocab_mask = self.answer_question(input_observation, input_observation_char, observation_id_list, input_quest, input_quest_char, use_model=\"online\")  # batch x time\n",
        "            positions_maxq = self.point_maxq_position(vocab_distribution, vocab_mask)\n",
        "            return positions_maxq  # batch\n",
        "\n",
        "    def get_qa_loss(self):\n",
        "        \"\"\"\n",
        "        Update neural model in agent. In this example we follow algorithm\n",
        "        of updating model in dqn with replay memory.\n",
        "        \"\"\"\n",
        "        if len(self.qa_replay_memory) < self.replay_batch_size:\n",
        "            return None\n",
        "        transitions = self.qa_replay_memory.sample(self.replay_batch_size)\n",
        "        batch = qa_memory.qa_Transition(*zip(*transitions))\n",
        "\n",
        "        observation_list = batch.observation_list\n",
        "        quest_list = batch.quest_list\n",
        "        answer_strings = batch.answer_strings\n",
        "        answer_position = np.array(_words_to_ids(answer_strings, self.word2id))\n",
        "        groundtruth = to_pt(answer_position, self.use_cuda)  # batch\n",
        "\n",
        "        input_quest, input_quest_char, _ = self.get_agent_inputs(quest_list)\n",
        "        input_observation, input_observation_char, observation_id_list =  self.get_agent_inputs(observation_list)\n",
        "\n",
        "        answer_distribution, _, _ = self.answer_question(input_observation, input_observation_char, observation_id_list, input_quest, input_quest_char, use_model=\"online\")  # batch x vocab\n",
        "\n",
        "        batch_loss = NegativeLogLoss(answer_distribution, groundtruth)  # batch\n",
        "        return torch.mean(batch_loss)\n",
        "\n",
        "    def update_qa(self):\n",
        "        # update neural model by replaying snapshots in replay memory\n",
        "        qa_loss = self.get_qa_loss()\n",
        "        if qa_loss is None:\n",
        "            return None\n",
        "        loss = qa_loss * self.qa_loss_lambda\n",
        "        # Backpropagate\n",
        "        self.online_net.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), self.clip_grad_norm)\n",
        "        self.optimizer.step()  # apply gradients\n",
        "        return to_np(torch.mean(qa_loss))\n",
        "\n",
        "    def finish_of_episode(self, episode_no, batch_size):\n",
        "        # Update target networt\n",
        "        if (episode_no + batch_size) % self.target_net_update_frequency <= episode_no % self.target_net_update_frequency:\n",
        "            self.update_target_net()\n",
        "        # decay lambdas\n",
        "        if episode_no < self.learn_start_from_this_episode:\n",
        "            return\n",
        "        if episode_no < self.epsilon_anneal_episodes + self.learn_start_from_this_episode:\n",
        "            self.epsilon -= (self.epsilon_anneal_from - self.epsilon_anneal_to) / float(self.epsilon_anneal_episodes)\n",
        "            self.epsilon = max(self.epsilon, 0.0)\n",
        "        if episode_no < self.revisit_counting_lambda_anneal_episodes + self.learn_start_from_this_episode:\n",
        "            self.revisit_counting_lambda -= (self.revisit_counting_lambda_anneal_from - self.revisit_counting_lambda_anneal_to) / float(self.revisit_counting_lambda_anneal_episodes)\n",
        "            self.revisit_counting_lambda = max(self.epsilon, 0.0)\n",
        "\n",
        "    def reset_binarized_counter(self, batch_size):\n",
        "        self.binarized_counter_dict = [{} for _ in range(batch_size)]\n",
        "\n",
        "    def get_binarized_count(self, observation_strings, update=True):\n",
        "        count_rewards = []\n",
        "        batch_size = len(observation_strings)\n",
        "        for i in range(batch_size):\n",
        "            key = observation_strings[i]\n",
        "            if key not in self.binarized_counter_dict[i]:\n",
        "                self.binarized_counter_dict[i][key] = 0.0\n",
        "            if update:\n",
        "                self.binarized_counter_dict[i][key] += 1.0\n",
        "            r = self.binarized_counter_dict[i][key]\n",
        "            r = float(r == 1.0)\n",
        "            count_rewards.append(r)\n",
        "        return count_rewards\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}