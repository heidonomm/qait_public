{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import generic\n",
        "import reward_helper\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from os.path import join as pjoin\n",
        "import gym\n",
        "import textworld\n",
        "from textworld.gym import register_games, make_batch\n",
        "from query import process_facts\n",
        "\n",
        "request_infos = textworld.EnvInfos(description=True,\n",
        "                                   inventory=True,\n",
        "                                   verbs=True,\n",
        "                                   location_names=True,\n",
        "                                   location_nouns=True,\n",
        "                                   location_adjs=True,\n",
        "                                   object_names=True,\n",
        "                                   object_nouns=True,\n",
        "                                   object_adjs=True,\n",
        "                                   facts=True,\n",
        "                                   last_action=True,\n",
        "                                   game=True,\n",
        "                                   admissible_commands=True,\n",
        "                                   extras=[\"object_locations\", \"object_attributes\", \"uuid\"])\n",
        "\n",
        "\n",
        "def evaluate(data_path, agent):\n",
        "\n",
        "    eval_data_path = pjoin(data_path, agent.eval_data_path)\n",
        "\n",
        "    with open(eval_data_path) as f:\n",
        "        data = json.load(f)\n",
        "    data = data[agent.question_type]\n",
        "    data = data[\"random_map\"] if agent.random_map else data[\"fixed_map\"]\n",
        "\n",
        "    print_qa_reward, print_sufficient_info_reward = [], []\n",
        "    for game_path in tqdm(data):\n",
        "        game_file_path = pjoin(data_path, game_path)\n",
        "        assert os.path.exists(game_file_path), \"Oh no! game path %s does not exist!\" % game_file_path\n",
        "        env_id = register_games([game_file_path], request_infos=request_infos)\n",
        "        env_id = make_batch(env_id, batch_size=agent.eval_batch_size, parallel=True)\n",
        "        env = gym.make(env_id)\n",
        "\n",
        "        data_questions = [item[\"question\"] for item in data[game_path]]\n",
        "        data_answers = [item[\"answer\"] for item in data[game_path]]\n",
        "        data_entities = [item[\"entity\"] for item in data[game_path]]\n",
        "        if agent.question_type == \"attribute\":\n",
        "            data_attributes = [item[\"attribute\"] for item in data[game_path]]\n",
        "\n",
        "        for q_no in range(len(data_questions)):\n",
        "            questions = data_questions[q_no: q_no + 1]\n",
        "            answers = data_answers[q_no: q_no + 1]\n",
        "            reward_helper_info = {\"_entities\": data_entities[q_no: q_no + 1],\n",
        "                                  \"_answers\": data_answers[q_no: q_no + 1]}\n",
        "            if agent.question_type == \"attribute\":\n",
        "                reward_helper_info[\"_attributes\"] = data_attributes[q_no: q_no + 1]\n",
        "\n",
        "            obs, infos = env.reset()\n",
        "            batch_size = len(obs)\n",
        "            agent.eval()\n",
        "            agent.init(obs, infos)\n",
        "            # get inputs\n",
        "            commands, last_facts, init_facts = [], [], []\n",
        "            commands_per_step, game_facts_cache = [], []\n",
        "            for i in range(batch_size):\n",
        "                commands.append(\"restart\")\n",
        "                last_facts.append(None)\n",
        "                init_facts.append(None)\n",
        "                game_facts_cache.append([])\n",
        "                commands_per_step.append([\"restart\"])\n",
        "\n",
        "            observation_strings, possible_words = agent.get_game_info_at_certain_step(obs, infos)\n",
        "            observation_strings = [a + \" <|> \" + item for a, item in zip(commands, observation_strings)]\n",
        "            input_quest, input_quest_char, _ = agent.get_agent_inputs(questions)\n",
        "\n",
        "            transition_cache = []\n",
        "\n",
        "            for step_no in range(agent.eval_max_nb_steps_per_episode):\n",
        "                # update answerer input\n",
        "                for i in range(batch_size):\n",
        "                    if agent.not_finished_yet[i] == 1:\n",
        "                        agent.naozi.push_one(i, copy.copy(observation_strings[i]))\n",
        "                    if agent.prev_step_is_still_interacting[i] == 1:\n",
        "                        new_facts = process_facts(last_facts[i], infos[\"game\"][i], infos[\"facts\"][i], infos[\"last_action\"][i], commands[i])\n",
        "                        game_facts_cache[i].append(new_facts)  # info used in reward computing of existence question\n",
        "                        last_facts[i] = new_facts\n",
        "                        if step_no == 0:\n",
        "                            init_facts[i] = copy.copy(new_facts)\n",
        "\n",
        "                observation_strings_w_history = agent.naozi.get()\n",
        "                input_observation, input_observation_char, _ =  agent.get_agent_inputs(observation_strings_w_history)\n",
        "                commands, replay_info = agent.act(obs, infos, input_observation, input_observation_char, input_quest, input_quest_char, possible_words, random=False)\n",
        "                for i in range(batch_size):\n",
        "                    commands_per_step[i].append(commands[i])\n",
        "\n",
        "                replay_info = [observation_strings_w_history, questions, possible_words] + replay_info\n",
        "                transition_cache.append(replay_info)\n",
        "\n",
        "                obs, _, _, infos = env.step(commands)\n",
        "                # possible words no not depend on history, because one can only interact with what is currently accessible\n",
        "                observation_strings, possible_words = agent.get_game_info_at_certain_step(obs, infos)\n",
        "                observation_strings = [a + \" <|> \" + item for a, item in zip(commands, observation_strings)]\n",
        "\n",
        "                if (step_no == agent.eval_max_nb_steps_per_episode - 1 ) or (step_no > 0 and np.sum(generic.to_np(replay_info[-1])) == 0):\n",
        "                    break\n",
        "\n",
        "            # The agent has exhausted all steps, now answer question.\n",
        "            answerer_input = agent.naozi.get()\n",
        "            answerer_input_observation, answerer_input_observation_char, answerer_observation_ids =  agent.get_agent_inputs(answerer_input)\n",
        "\n",
        "            chosen_word_indices = agent.answer_question_act_greedy(answerer_input_observation, answerer_input_observation_char, answerer_observation_ids, input_quest, input_quest_char)  # batch\n",
        "            chosen_word_indices_np = generic.to_np(chosen_word_indices)\n",
        "            chosen_answers = [agent.word_vocab[item] for item in chosen_word_indices_np]\n",
        "\n",
        "            # rewards\n",
        "            # qa reward\n",
        "            qa_reward_np = reward_helper.get_qa_reward(answers, chosen_answers)\n",
        "            # sufficient info rewards\n",
        "            masks = [item[-1] for item in transition_cache]\n",
        "            masks_np = [generic.to_np(item) for item in masks]\n",
        "            # 1 1 0 0 0 --> 1 1 0 0 0 0\n",
        "            game_finishing_mask = np.stack(masks_np + [np.zeros((batch_size,))], 0)  # game step+1 x batch size\n",
        "            # 1 1 0 0 0 0 --> 0 1 0 0 0\n",
        "            game_finishing_mask = game_finishing_mask[:-1, :] - game_finishing_mask[1:, :]  # game step x batch size\n",
        "            \n",
        "            if agent.question_type == \"location\":\n",
        "                # sufficient info reward: location question\n",
        "                reward_helper_info[\"observation_before_finish\"] = answerer_input\n",
        "                reward_helper_info[\"game_finishing_mask\"] = game_finishing_mask\n",
        "                sufficient_info_reward_np = reward_helper.get_sufficient_info_reward_location(reward_helper_info)\n",
        "            elif agent.question_type == \"existence\":\n",
        "                # sufficient info reward: existence question\n",
        "                reward_helper_info[\"observation_before_finish\"] = answerer_input\n",
        "                reward_helper_info[\"game_facts_per_step\"] = game_facts_cache  # facts before issuing command (we want to stop at correct state)\n",
        "                reward_helper_info[\"init_game_facts\"] = init_facts\n",
        "                reward_helper_info[\"full_facts\"] = infos[\"facts\"]\n",
        "                reward_helper_info[\"answers\"] = answers\n",
        "                reward_helper_info[\"game_finishing_mask\"] = game_finishing_mask\n",
        "                sufficient_info_reward_np = reward_helper.get_sufficient_info_reward_existence(reward_helper_info)\n",
        "            elif agent.question_type == \"attribute\":\n",
        "                # sufficient info reward: attribute question\n",
        "                reward_helper_info[\"answers\"] = answers\n",
        "                reward_helper_info[\"game_facts_per_step\"] = game_facts_cache  # facts before and after issuing commands (we want to compare the differnce)\n",
        "                reward_helper_info[\"init_game_facts\"] = init_facts\n",
        "                reward_helper_info[\"full_facts\"] = infos[\"facts\"]\n",
        "                reward_helper_info[\"commands_per_step\"] = commands_per_step  # commands before and after issuing commands (we want to compare the differnce)\n",
        "                reward_helper_info[\"game_finishing_mask\"] = game_finishing_mask\n",
        "                sufficient_info_reward_np = reward_helper.get_sufficient_info_reward_attribute(reward_helper_info)\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            r_qa = np.mean(qa_reward_np)\n",
        "            r_sufficient_info = np.mean(np.sum(sufficient_info_reward_np, -1))\n",
        "            print_qa_reward.append(r_qa)\n",
        "            print_sufficient_info_reward.append(r_sufficient_info)\n",
        "        env.close()\n",
        "\n",
        "    print(\"===== Eval =====: qa acc: {:2.3f} | correct state: {:2.3f}\".format(np.mean(print_qa_reward), np.mean(print_sufficient_info_reward)))\n",
        "    return np.mean(print_qa_reward), np.mean(print_sufficient_info_reward)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}