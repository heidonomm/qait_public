{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import random\n",
        "import uuid\n",
        "import os\n",
        "import time\n",
        "import multiprocessing as mp\n",
        "from os.path import join as pjoin\n",
        "missing_words = set()\n",
        "\n",
        "\n",
        "def to_np(x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return x\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "\n",
        "def to_pt(np_matrix, enable_cuda=False, type='long'):\n",
        "    if type == 'long':\n",
        "        if enable_cuda:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.LongTensor).cuda())\n",
        "        else:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix.copy()).type(torch.LongTensor))\n",
        "    elif type == 'float':\n",
        "        if enable_cuda:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.FloatTensor).cuda())\n",
        "        else:\n",
        "            return torch.autograd.Variable(torch.from_numpy(np_matrix.copy()).type(torch.FloatTensor))\n",
        "\n",
        "\n",
        "def _word_to_id(word, word2id):\n",
        "    try:\n",
        "        return word2id[word]\n",
        "    except KeyError:\n",
        "        key = word + \"_\" + str(len(word2id))\n",
        "        if key not in missing_words:\n",
        "            print(\"Warning... %s is not in vocab, vocab size is %d...\" % (word, len(word2id)))\n",
        "            missing_words.add(key)\n",
        "            with open(\"missing_words.txt\", 'a+') as outfile:\n",
        "                outfile.write(key + '\\n')\n",
        "                outfile.flush()\n",
        "        return 1\n",
        "\n",
        "\n",
        "def _words_to_ids(words, word2id):\n",
        "    ids = []\n",
        "    for word in words:\n",
        "        ids.append(_word_to_id(word, word2id))\n",
        "    return ids\n",
        "\n",
        "\n",
        "def preproc(s, tokenizer=None):\n",
        "    if s is None:\n",
        "        return \"nothing\"\n",
        "    if \"$$$$$$$\" in s:\n",
        "        s = s.split(\"$$$$$$$\")[-1]\n",
        "    if \"are carrying:\" in s:\n",
        "        s = \" -= inventory =- \" + s\n",
        "    s = s.replace(\"\\n\", ' ')\n",
        "    if s.strip() == \"\":\n",
        "        return \"nothing\"\n",
        "    s = s.strip()\n",
        "    if len(s) == 0:\n",
        "        return \"nothing\"\n",
        "    s = \" \".join([t.text for t in tokenizer(s)])\n",
        "    s = s.lower()\n",
        "    return s\n",
        "\n",
        "\n",
        "def max_len(list_of_list):\n",
        "    return max(map(len, list_of_list))\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32', value=0.):\n",
        "    '''\n",
        "    Partially borrowed from Keras\n",
        "    # Arguments\n",
        "        sequences: list of lists where each element is a sequence\n",
        "        maxlen: int, maximum length\n",
        "        dtype: type to cast the resulting sequence.\n",
        "        value: float, value to pad the sequences to the desired value.\n",
        "    # Returns\n",
        "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
        "    '''\n",
        "    lengths = [len(s) for s in sequences]\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if len(s) == 0:\n",
        "            continue  # empty list was found\n",
        "        # pre truncating\n",
        "        trunc = s[-maxlen:]\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "        # post padding\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "    return x\n",
        "\n",
        "\n",
        "def ez_gather_dim_1(input, index):\n",
        "    if len(input.size()) == len(index.size()):\n",
        "        return input.gather(1, index)\n",
        "    res = []\n",
        "    for i in range(input.size(0)):\n",
        "        res.append(input[i][index[i][0]])\n",
        "    return torch.stack(res, 0)\n",
        "\n",
        "\n",
        "def list_of_token_list_to_char_input(list_of_token_list, char2id):\n",
        "    batch_size = len(list_of_token_list)\n",
        "    max_token_number = max_len(list_of_token_list)\n",
        "    max_char_number = max([max_len(item) for item in list_of_token_list])\n",
        "    if max_char_number < 6:\n",
        "        max_char_number = 6\n",
        "    res = np.zeros((batch_size, max_token_number, max_char_number), dtype='int32')\n",
        "    for i in range(batch_size):\n",
        "        for j in range(len(list_of_token_list[i])):\n",
        "            for k in range(len(list_of_token_list[i][j])):\n",
        "                res[i][j][k] = _word_to_id(list_of_token_list[i][j][k], char2id)\n",
        "    return res\n",
        "\n",
        "\n",
        "class HistoryScoreCache(object):\n",
        "\n",
        "    def __init__(self, capacity=1):\n",
        "        self.capacity = capacity\n",
        "        self.reset()\n",
        "\n",
        "    def push(self, stuff):\n",
        "        \"\"\"stuff is float.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(stuff)\n",
        "        else:\n",
        "            self.memory = self.memory[1:] + [stuff]\n",
        "\n",
        "    def get_avg(self):\n",
        "        return np.mean(np.array(self.memory))\n",
        "\n",
        "    def reset(self):\n",
        "        self.memory = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class ObservationPool(object):\n",
        "\n",
        "    def __init__(self, capacity=1):\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def identical_with_history(self, new_stuff, list_of_old_stuff):\n",
        "        new_obs = new_stuff.split(\"<|>\")[1].strip()\n",
        "        new_feedback = new_stuff.split(\"<|>\")[2].strip()\n",
        "        for i in range(len(list_of_old_stuff)):\n",
        "            if new_stuff == list_of_old_stuff[i]:\n",
        "                return True\n",
        "            # prev_act <|> obs <|> feedback\n",
        "            # if obs and feedback are seen before, drop it\n",
        "            if new_obs in list_of_old_stuff[i] and new_feedback in list_of_old_stuff[i]:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def push_batch(self, stuff):\n",
        "        assert len(stuff) == len(self.memory)\n",
        "        for i in range(len(stuff)):\n",
        "            if not self.identical_with_history(stuff[i], self.memory[i]):\n",
        "                self.memory[i].append(stuff[i])\n",
        "            if len(self.memory[i]) > self.capacity:\n",
        "                self.memory[i] = self.memory[i][-self.capacity:]\n",
        "\n",
        "    def push_one(self, which, stuff):\n",
        "        assert which < len(self.memory)\n",
        "        if not self.identical_with_history(stuff, self.memory[which]):\n",
        "            self.memory[which].append(stuff)\n",
        "        if len(self.memory[which]) > self.capacity:\n",
        "            self.memory[which] = self.memory[which][-self.capacity:]\n",
        "\n",
        "    def get_last(self):\n",
        "        return [item[-1] for item in self.memory]\n",
        "\n",
        "    def get(self, which=None):\n",
        "        if which is not None:\n",
        "            assert which < len(self.memory)\n",
        "            # prev_act <|> obs <|> feedback\n",
        "            output = \" <|> \".join(self.memory[which])\n",
        "            return output\n",
        "\n",
        "        output = []\n",
        "        for i in range(len(self.memory)):\n",
        "            output.append(\" <|> \".join(self.memory[i]))\n",
        "        return output\n",
        "\n",
        "    def get_sent_list(self):\n",
        "        return copy.copy(self.memory)\n",
        "\n",
        "    def reset(self, batch_size):\n",
        "        self.memory = []\n",
        "        for _ in range(batch_size):\n",
        "            self.memory.append([])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}