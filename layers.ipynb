{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import math\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def compute_mask(x):\n",
        "    mask = torch.ne(x, 0).float()\n",
        "    if x.is_cuda:\n",
        "        mask = mask.cuda()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def masked_softmax(x, m=None, axis=-1):\n",
        "    '''\n",
        "    Softmax with mask (optional)\n",
        "    '''\n",
        "    x = torch.clamp(x, min=-15.0, max=15.0)\n",
        "    if m is not None:\n",
        "        m = m.float()\n",
        "        x = x * m\n",
        "    e_x = torch.exp(x - torch.max(x, dim=axis, keepdim=True)[0])\n",
        "    if m is not None:\n",
        "        e_x = e_x * m\n",
        "    softmax = e_x / (torch.sum(e_x, dim=axis, keepdim=True) + 1e-6)\n",
        "    return softmax\n",
        "\n",
        "\n",
        "def masked_mean(x, m=None, dim=-1):\n",
        "    \"\"\"\n",
        "        mean pooling when there're paddings\n",
        "        input:  tensor: batch x time x h\n",
        "                mask:   batch x time\n",
        "        output: tensor: batch x h\n",
        "    \"\"\"\n",
        "    if m is None:\n",
        "        return torch.mean(x, dim=dim)\n",
        "    mask_sum = torch.sum(m, dim=-1)  # batch\n",
        "    res = torch.sum(x, dim=1)  # batch x h\n",
        "    res = res / (mask_sum.unsqueeze(-1) + 1e-6)\n",
        "    return res\n",
        "\n",
        "\n",
        "def to_one_hot(y_true, n_classes):\n",
        "    y_onehot = torch.FloatTensor(y_true.size(0), n_classes)\n",
        "    if y_true.is_cuda:\n",
        "        y_onehot = y_onehot.cuda()\n",
        "    y_onehot.zero_()\n",
        "    y_onehot.scatter_(1, y_true, 1)\n",
        "    return y_onehot\n",
        "\n",
        "\n",
        "def NegativeLogLoss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Shape:\n",
        "        - y_pred:    batch x time\n",
        "        - y_true:    batch\n",
        "    \"\"\"\n",
        "    y_true_onehot = to_one_hot(y_true.unsqueeze(-1), y_pred.size(1))\n",
        "    P = y_true_onehot.squeeze(-1) * y_pred  # batch x time\n",
        "    P = torch.sum(P, dim=1)  # batch\n",
        "    gt_zero = torch.gt(P, 0.0).float()  # batch\n",
        "    epsilon = torch.le(P, 0.0).float() * 1e-8  # batch\n",
        "    log_P = torch.log(P + epsilon) * gt_zero  # batch\n",
        "    output = -log_P  # batch\n",
        "    return output\n",
        "\n",
        "\n",
        "def PosEncoder(x, min_timescale=1.0, max_timescale=1.0e4):\n",
        "    length = x.size(1)\n",
        "    channels = x.size(2)\n",
        "    signal = get_timing_signal(length, channels, min_timescale, max_timescale)\n",
        "\n",
        "    signal = signal.cuda() if x.is_cuda else signal\n",
        "    return x + signal\n",
        "\n",
        "def get_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n",
        "    position = torch.arange(length).type(torch.float32)\n",
        "    num_timescales = channels // 2\n",
        "    log_timescale_increment = (math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales)-1))\n",
        "    inv_timescales = min_timescale * torch.exp(\n",
        "            torch.arange(num_timescales).type(torch.float32) * -log_timescale_increment)\n",
        "    scaled_time = position.unsqueeze(1) * inv_timescales.unsqueeze(0)\n",
        "    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim = 1)\n",
        "    m = torch.nn.ZeroPad2d((0, (channels % 2), 0, 0))\n",
        "    signal = m(signal)\n",
        "    signal = signal.view(1, length, channels)\n",
        "    return signal\n",
        "\n",
        "\n",
        "class LayerNorm(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = torch.nn.Parameter(torch.ones(input_dim))\n",
        "        self.beta = torch.nn.Parameter(torch.zeros(input_dim))\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x:        nbatch x hidden\n",
        "        # mask:     nbatch\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = torch.sqrt(x.var(dim=1, keepdim=True) + self.eps)\n",
        "        output = self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "        return output * mask.unsqueeze(1)\n",
        "\n",
        "\n",
        "class H5EmbeddingManager(object):\n",
        "    def __init__(self, h5_path):\n",
        "        f = h5py.File(h5_path, 'r')\n",
        "        self.W = np.array(f['embedding'])\n",
        "        print(\"embedding data type=%s, shape=%s\" % (type(self.W), self.W.shape))\n",
        "        self.id2word = f['words_flatten'][0].split('\\n')\n",
        "        self.word2id = dict(zip(self.id2word, range(len(self.id2word))))\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        item_type = type(item)\n",
        "        if item_type is str:\n",
        "            index = self.word2id[item]\n",
        "            embs = self.W[index]\n",
        "            return embs\n",
        "        else:\n",
        "            raise RuntimeError(\"don't support type: %s\" % type(item))\n",
        "\n",
        "    def word_embedding_initialize(self, words_list, dim_size=300, scale=0.1, oov_init='random'):\n",
        "        shape = (len(words_list), dim_size)\n",
        "        np.random.seed(42)\n",
        "        if 'zero' == oov_init:\n",
        "            W2V = np.zeros(shape, dtype='float32')\n",
        "        elif 'one' == oov_init:\n",
        "            W2V = np.ones(shape, dtype='float32')\n",
        "        else:\n",
        "            W2V = np.random.uniform(low=-scale, high=scale, size=shape).astype('float32')\n",
        "        W2V[0, :] = 0\n",
        "        in_vocab = np.ones(shape[0], dtype=np.bool)\n",
        "        word_ids = []\n",
        "        for i, word in enumerate(words_list):\n",
        "            if word in self.word2id:\n",
        "                word_ids.append(self.word2id[word])\n",
        "            else:\n",
        "                in_vocab[i] = False\n",
        "        W2V[in_vocab] = self.W[np.array(word_ids, dtype='int32')][:, :dim_size]\n",
        "        return W2V\n",
        "\n",
        "\n",
        "class Embedding(torch.nn.Module):\n",
        "    '''\n",
        "    inputs: x:          batch x ...\n",
        "    outputs:embedding:  batch x ... x emb\n",
        "            mask:       batch x ...\n",
        "    '''\n",
        "\n",
        "    def __init__(self, embedding_size, vocab_size, dropout_rate=0.0, trainable=True, id2word=None,\n",
        "                 embedding_oov_init='random', load_pretrained=False, pretrained_embedding_path=None):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.id2word = id2word\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.load_pretrained = load_pretrained\n",
        "        self.embedding_oov_init = embedding_oov_init\n",
        "        self.pretrained_embedding_path = pretrained_embedding_path\n",
        "        self.trainable = trainable\n",
        "        self.embedding_layer = torch.nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=0)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_embedding_matrix = self.embedding_init()\n",
        "        if self.embedding_layer.weight.is_cuda:\n",
        "            init_embedding_matrix = init_embedding_matrix.cuda()\n",
        "        self.embedding_layer.weight = torch.nn.Parameter(init_embedding_matrix)\n",
        "        if not self.trainable:\n",
        "            self.embedding_layer.weight.requires_grad = False\n",
        "\n",
        "    def embedding_init(self):\n",
        "        # Embeddings\n",
        "        if self.load_pretrained is False:\n",
        "            word_embedding_init = np.random.uniform(low=-0.05, high=0.05, size=(self.vocab_size, self.embedding_size))\n",
        "            word_embedding_init[0, :] = 0\n",
        "        else:\n",
        "            embedding_initr = H5EmbeddingManager(self.pretrained_embedding_path)\n",
        "            word_embedding_init = embedding_initr.word_embedding_initialize(self.id2word,\n",
        "                                                                            dim_size=self.embedding_size,\n",
        "                                                                            oov_init=self.embedding_oov_init)\n",
        "            del embedding_initr\n",
        "        word_embedding_init = torch.from_numpy(word_embedding_init).float()\n",
        "        return word_embedding_init\n",
        "\n",
        "    def compute_mask(self, x):\n",
        "        mask = torch.ne(x, 0).float()\n",
        "        if x.is_cuda:\n",
        "            mask = mask.cuda()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embedding_layer(x)  # batch x time x emb\n",
        "        embeddings = F.dropout(embeddings, p=self.dropout_rate, training=self.training)\n",
        "        mask = self.compute_mask(x)  # batch x time\n",
        "        return embeddings, mask\n",
        "\n",
        "\n",
        "class NoisyLinear(torch.nn.Module):\n",
        "    # Factorised NoisyLinear layer with bias\n",
        "    def __init__(self, in_features, out_features, std_init=0.5):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "        self.weight_mu = torch.nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.weight_sigma = torch.nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
        "        self.bias_mu = torch.nn.Parameter(torch.empty(out_features))\n",
        "        self.bias_sigma = torch.nn.Parameter(torch.empty(out_features))\n",
        "        self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "        self._zero_noise = False\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / math.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
        "\n",
        "    def _scale_noise(self, size):\n",
        "        x = torch.randn(size)\n",
        "        return x.sign().mul_(x.abs().sqrt_())\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def zero_noise(self):\n",
        "        self._zero_noise = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            if self._zero_noise is True:\n",
        "                return F.linear(input, self.weight_mu, self.bias_mu)\n",
        "            else:\n",
        "                return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon, self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
        "        else:\n",
        "            return F.linear(input, self.weight_mu, self.bias_mu)\n",
        "\n",
        "\n",
        "class DepthwiseSeparableConv(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k, bias=True):\n",
        "        super().__init__()\n",
        "        self.depthwise_conv = torch.nn.Conv1d(in_channels=in_ch, out_channels=in_ch, kernel_size=k, groups=in_ch, padding=k // 2, bias=False)\n",
        "        self.pointwise_conv = torch.nn.Conv1d(in_channels=in_ch, out_channels=out_ch, kernel_size=1, padding=0, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1,2)\n",
        "        res = torch.relu(self.pointwise_conv(self.depthwise_conv(x)))\n",
        "        res = res.transpose(1,2)\n",
        "        return res\n",
        "\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, block_hidden_dim, n_head, dropout):\n",
        "        super().__init__()\n",
        "        self.block_hidden_dim = block_hidden_dim\n",
        "        self.n_head = n_head\n",
        "        self.dropout = dropout\n",
        "        self.key_linear = torch.nn.Linear(block_hidden_dim, block_hidden_dim, bias=False)\n",
        "        self.value_linear = torch.nn.Linear(block_hidden_dim, block_hidden_dim, bias=False)\n",
        "        self.query_linear = torch.nn.Linear(block_hidden_dim, block_hidden_dim, bias=False)\n",
        "        bias = torch.empty(1)\n",
        "        torch.nn.init.constant_(bias, 0)\n",
        "        self.bias = torch.nn.Parameter(bias)\n",
        "\n",
        "    def forward(self, queries, query_mask, keys, values):\n",
        "\n",
        "        query = self.query_linear(queries)\n",
        "        key = self.key_linear(keys)\n",
        "        value = self.value_linear(values)\n",
        "        Q = self.split_last_dim(query, self.n_head)\n",
        "        K = self.split_last_dim(key, self.n_head)\n",
        "        V = self.split_last_dim(value, self.n_head)\n",
        "        \n",
        "        assert self.block_hidden_dim % self.n_head == 0\n",
        "        key_depth_per_head = self.block_hidden_dim // self.n_head\n",
        "        Q *= key_depth_per_head**-0.5\n",
        "        x = self.dot_product_attention(Q, K, V, mask=query_mask)\n",
        "        return self.combine_last_two_dim(x.permute(0, 2, 1, 3))\n",
        "\n",
        "    def dot_product_attention(self, q, k ,v, bias=False, mask=None):\n",
        "        \"\"\"dot-product attention.\n",
        "        Args:\n",
        "        q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
        "        k: a Tensor with shape [batch, heads, length_kv, depth_k]\n",
        "        v: a Tensor with shape [batch, heads, length_kv, depth_v]\n",
        "        bias: bias Tensor (see attention_bias())\n",
        "        is_training: a bool of training\n",
        "        scope: an optional string\n",
        "        Returns:\n",
        "        A Tensor.\n",
        "        \"\"\"\n",
        "        logits = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
        "        if bias:\n",
        "            logits += self.bias\n",
        "        if mask is not None:\n",
        "            # shapes = [x if x != None else -1 for x in list(logits.size())]\n",
        "            # mask = mask.view(shapes[0], 1, 1, shapes[-1])\n",
        "            mask = mask.unsqueeze(1)\n",
        "        weights = masked_softmax(logits, mask, axis=-1)\n",
        "        # dropping out the attention links for each of the heads\n",
        "        weights = F.dropout(weights, p=self.dropout, training=self.training)\n",
        "        return torch.matmul(weights, v)\n",
        "\n",
        "    def split_last_dim(self, x, n):\n",
        "        \"\"\"Reshape x so that the last dimension becomes two dimensions.\n",
        "        The first of these two dimensions is n.\n",
        "        Args:\n",
        "        x: a Tensor with shape [..., m]\n",
        "        n: an integer.\n",
        "        Returns:\n",
        "        a Tensor with shape [..., n, m/n]\n",
        "        \"\"\"\n",
        "        old_shape = list(x.size())\n",
        "        last = old_shape[-1]\n",
        "        new_shape = old_shape[:-1] + [n] + [last // n if last else None]\n",
        "        ret = x.view(new_shape)\n",
        "        return ret.permute(0, 2, 1, 3)\n",
        "\n",
        "    def combine_last_two_dim(self, x):\n",
        "        \"\"\"Reshape x so that the last two dimension become one.\n",
        "        Args:\n",
        "        x: a Tensor with shape [..., a, b]\n",
        "        Returns:\n",
        "        a Tensor with shape [..., ab]\n",
        "        \"\"\"\n",
        "        old_shape = list(x.size())\n",
        "        a, b = old_shape[-2:]\n",
        "        new_shape = old_shape[:-2] + [a * b if a and b else None]\n",
        "        ret = x.contiguous().view(new_shape)\n",
        "        return ret\n",
        "\n",
        "\n",
        "class EncoderBlock(torch.nn.Module):\n",
        "    def __init__(self, conv_num, ch_num, k, block_hidden_dim, n_head, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.convs = torch.nn.ModuleList([DepthwiseSeparableConv(ch_num, ch_num, k) for _ in range(conv_num)])\n",
        "        self.self_att = SelfAttention(block_hidden_dim, n_head, dropout)\n",
        "        self.FFN_1 = torch.nn.Linear(ch_num, ch_num)\n",
        "        self.FFN_2 = torch.nn.Linear(ch_num, ch_num)\n",
        "        self.norm_C = torch.nn.ModuleList([torch.nn.LayerNorm(block_hidden_dim) for _ in range(conv_num)])\n",
        "        self.norm_1 = torch.nn.LayerNorm(block_hidden_dim)\n",
        "        self.norm_2 = torch.nn.LayerNorm(block_hidden_dim)\n",
        "        self.conv_num = conv_num\n",
        "\n",
        "    def forward(self, x, mask, self_att_mask, l, blks):\n",
        "        total_layers = (self.conv_num + 2) * blks\n",
        "        # conv layers\n",
        "        out = PosEncoder(x)\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            res = out\n",
        "            out = self.norm_C[i](out)\n",
        "            if (i) % 2 == 0:\n",
        "                out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "            out = conv(out)\n",
        "            out = out * mask.unsqueeze(-1)\n",
        "            out = self.layer_dropout(out, res, self.dropout * float(l) / total_layers)\n",
        "            l += 1\n",
        "        res = out\n",
        "        out = self.norm_1(out)\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        # self attention\n",
        "        out = self.self_att(out, self_att_mask, out, out)\n",
        "        out = out * mask.unsqueeze(-1)\n",
        "        out = self.layer_dropout(out, res, self.dropout * float(l) / total_layers)\n",
        "        l += 1\n",
        "        res = out\n",
        "        out = self.norm_2(out)\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        # fully connected layers\n",
        "        out = self.FFN_1(out)\n",
        "        out = torch.relu(out)\n",
        "        out = self.FFN_2(out)\n",
        "        out = out * mask.unsqueeze(-1)\n",
        "        out = self.layer_dropout(out, res, self.dropout * float(l) / total_layers)\n",
        "        l += 1\n",
        "        return out\n",
        "\n",
        "    def layer_dropout(self, inputs, residual, dropout):\n",
        "        if self.training == True:\n",
        "            pred = torch.empty(1).uniform_(0, 1) < dropout\n",
        "            if pred:\n",
        "                return residual\n",
        "            else:\n",
        "                return F.dropout(inputs, dropout, training=self.training) + residual\n",
        "        else:\n",
        "            return inputs + residual\n",
        "\n",
        "\n",
        "class CQAttention(torch.nn.Module):\n",
        "    def __init__(self, block_hidden_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        w4C = torch.empty(block_hidden_dim, 1)\n",
        "        w4Q = torch.empty(block_hidden_dim, 1)\n",
        "        w4mlu = torch.empty(1, 1, block_hidden_dim)\n",
        "        torch.nn.init.xavier_uniform_(w4C)\n",
        "        torch.nn.init.xavier_uniform_(w4Q)\n",
        "        torch.nn.init.xavier_uniform_(w4mlu)\n",
        "        self.w4C = torch.nn.Parameter(w4C)\n",
        "        self.w4Q = torch.nn.Parameter(w4Q)\n",
        "        self.w4mlu = torch.nn.Parameter(w4mlu)\n",
        "\n",
        "        bias = torch.empty(1)\n",
        "        torch.nn.init.constant_(bias, 0)\n",
        "        self.bias = torch.nn.Parameter(bias)\n",
        "\n",
        "    def forward(self, C, Q, Cmask, Qmask):\n",
        "        S = self.trilinear_for_attention(C, Q)\n",
        "        Cmask = Cmask.unsqueeze(-1)\n",
        "        Qmask = Qmask.unsqueeze(1)\n",
        "        S1 = masked_softmax(S, Qmask, axis=2)\n",
        "        S2 = masked_softmax(S, Cmask, axis=1)\n",
        "        A = torch.bmm(S1, Q)\n",
        "        B = torch.bmm(torch.bmm(S1, S2.transpose(1, 2)), C)\n",
        "        out = torch.cat([C, A, torch.mul(C, A), torch.mul(C, B)], dim=2)\n",
        "        return out\n",
        "\n",
        "    def trilinear_for_attention(self, C, Q):\n",
        "        C = F.dropout(C, p=self.dropout, training=self.training)\n",
        "        Q = F.dropout(Q, p=self.dropout, training=self.training)\n",
        "        max_q_len = Q.size(-2)\n",
        "        max_context_len = C.size(-2)\n",
        "        subres0 = torch.matmul(C, self.w4C).expand([-1, -1, max_q_len])\n",
        "        subres1 = torch.matmul(Q, self.w4Q).transpose(1, 2).expand([-1, max_context_len, -1])\n",
        "        subres2 = torch.matmul(C * self.w4mlu, Q.transpose(1, 2))\n",
        "        res = subres0 + subres1 + subres2\n",
        "        res += self.bias\n",
        "        return res\n",
        "\n",
        "\n",
        "class AnswerPointer(torch.nn.Module):\n",
        "    def __init__(self, block_hidden_dim, noisy_net=False):\n",
        "        super().__init__()\n",
        "        self.noisy_net = noisy_net\n",
        "        if self.noisy_net:\n",
        "            self.w_1 = NoisyLinear(block_hidden_dim * 2, 1)\n",
        "            self.w_1_advantage = NoisyLinear(block_hidden_dim * 2, block_hidden_dim)\n",
        "            self.w_2 = NoisyLinear(block_hidden_dim, 1)\n",
        "        else:\n",
        "            self.w_1 = torch.nn.Linear(block_hidden_dim * 2, 1)\n",
        "            self.w_1_advantage = torch.nn.Linear(block_hidden_dim * 2, block_hidden_dim)\n",
        "            self.w_2 = torch.nn.Linear(block_hidden_dim, 1)\n",
        "\n",
        "    def forward(self, M1, M2, mask):\n",
        "        X_concat = torch.cat([M1, M2], dim=-1)\n",
        "        X = torch.relu(self.w_1(X_concat))\n",
        "        X_advantage = torch.relu(self.w_1_advantage(X_concat))\n",
        "        X = X * mask.unsqueeze(-1)\n",
        "        X = X + X_advantage - X_advantage.mean(-1, keepdim=True)  # combine streams\n",
        "        X = X * mask.unsqueeze(-1)\n",
        "        Y = self.w_2(X).squeeze()\n",
        "        Y = Y * mask\n",
        "        return Y\n",
        "\n",
        "    def reset_noise(self):\n",
        "        if self.noisy_net:\n",
        "            self.w_1.reset_noise()\n",
        "            self.w_1_advantage.reset_noise()\n",
        "            self.w_2.reset_noise()\n",
        "    \n",
        "    def zero_noise(self):\n",
        "        if self.noisy_net:\n",
        "            self.w_1.zero_noise()\n",
        "            self.w_1_advantage.zero_noise()\n",
        "            self.w_2.zero_noise()\n",
        "\n",
        "\n",
        "class Highway(torch.nn.Module):\n",
        "    def __init__(self, layer_num, size, dropout=0):\n",
        "        super().__init__()\n",
        "        self.n = layer_num\n",
        "        self.dropout = dropout\n",
        "        self.linear = torch.nn.ModuleList([torch.nn.Linear(size, size) for _ in range(self.n)])\n",
        "        self.gate = torch.nn.ModuleList([torch.nn.Linear(size, size) for _ in range(self.n)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x: shape [batch_size, hidden_size, length]\n",
        "        for i in range(self.n):\n",
        "            gate = torch.sigmoid(self.gate[i](x))\n",
        "            nonlinear = self.linear[i](x)\n",
        "            nonlinear = F.dropout(nonlinear, p=self.dropout, training=self.training)\n",
        "            x = gate * nonlinear + (1 - gate) * x\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergeEmbeddings(torch.nn.Module):\n",
        "    def __init__(self, block_hidden_dim, word_emb_dim, char_emb_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(char_emb_dim, block_hidden_dim, kernel_size = (1, 5), padding=0, bias=True)\n",
        "        torch.nn.init.kaiming_normal_(self.conv2d.weight, nonlinearity='relu')\n",
        "\n",
        "        self.linear = torch.nn.Linear(word_emb_dim + block_hidden_dim, block_hidden_dim, bias=False)\n",
        "        self.high = Highway(2, size=block_hidden_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, word_emb, char_emb, mask=None):\n",
        "        char_emb = char_emb.permute(0, 3, 1, 2)  # batch x emb x time x nchar\n",
        "        char_emb = self.conv2d(char_emb)  # batch x block_hidden_dim x time x nchar-5+1\n",
        "        if mask is not None:\n",
        "            char_emb = char_emb * mask.unsqueeze(1).unsqueeze(-1)\n",
        "        char_emb = F.relu(char_emb)  # batch x block_hidden_dim x time x nchar-5+1\n",
        "        char_emb, _ = torch.max(char_emb, dim=3)  # batch x emb x time\n",
        "        char_emb = char_emb.permute(0, 2, 1)  # batch x time x emb\n",
        "        emb = torch.cat([char_emb, word_emb], dim=2)\n",
        "        emb = self.linear(emb)\n",
        "        emb = self.high(emb)\n",
        "        if mask is not None:\n",
        "            emb = emb * mask.unsqueeze(-1)\n",
        "        return emb\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}