{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import json\n",
        "import visdom\n",
        "import torch\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from os.path import join as pjoin\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "import gym\n",
        "import textworld\n",
        "from textworld.gym import register_game, make_batch2\n",
        "from agent import Agent\n",
        "import generic\n",
        "import reward_helper\n",
        "import game_generator\n",
        "import evaluate\n",
        "from query import process_facts\n",
        "\n",
        "request_infos = textworld.EnvInfos(description=True,\n",
        "                                   inventory=True,\n",
        "                                   verbs=True,\n",
        "                                   location_names=True,\n",
        "                                   location_nouns=True,\n",
        "                                   location_adjs=True,\n",
        "                                   object_names=True,\n",
        "                                   object_nouns=True,\n",
        "                                   object_adjs=True,\n",
        "                                   facts=True,\n",
        "                                   last_action=True,\n",
        "                                   game=True,\n",
        "                                   admissible_commands=True,\n",
        "                                   extras=[\"object_locations\", \"object_attributes\", \"uuid\"])\n",
        "\n",
        "\n",
        "def train(data_path):\n",
        "\n",
        "    time_1 = datetime.datetime.now()\n",
        "    agent = Agent()\n",
        "\n",
        "    # visdom\n",
        "    viz = visdom.Visdom()\n",
        "    plt_win = None\n",
        "    eval_plt_win = None\n",
        "    viz_avg_correct_state_acc, viz_avg_qa_acc = [], []\n",
        "    viz_eval_sufficient_info_reward, viz_eval_qa_reward = [], []\n",
        "\n",
        "    step_in_total = 0\n",
        "    running_avg_qa_reward = generic.HistoryScoreCache(capacity=500)\n",
        "    running_avg_sufficient_info_reward = generic.HistoryScoreCache(capacity=500)\n",
        "    running_avg_qa_loss = generic.HistoryScoreCache(capacity=500)\n",
        "    running_avg_correct_state_loss = generic.HistoryScoreCache(capacity=500)\n",
        "\n",
        "    output_dir, data_dir = \".\", \".\"\n",
        "    json_file_name = agent.experiment_tag.replace(\" \", \"_\")\n",
        "    best_sum_reward_so_far = 0.0\n",
        "    # load model from checkpoint\n",
        "    if agent.load_pretrained:\n",
        "        if os.path.exists(output_dir + \"/\" + agent.experiment_tag + \"_model.pt\"):\n",
        "            agent.load_pretrained_model(output_dir + \"/\" + agent.experiment_tag + \"_model.pt\")\n",
        "            agent.update_target_net()\n",
        "        elif os.path.exists(data_dir + \"/\" + agent.load_from_tag + \".pt\"):\n",
        "            agent.load_pretrained_model(data_dir + \"/\" + agent.load_from_tag + \".pt\")\n",
        "            agent.update_target_net()\n",
        "        else:\n",
        "            print(\"Failed to load pretrained model... couldn't find the checkpoint file...\")\n",
        "\n",
        "    # Create temporary folder for the generated games.\n",
        "    games_dir = tempfile.TemporaryDirectory(prefix=\"tw_games\")  # This is not deleted upon error. It would be better to use a with statement.\n",
        "    games_dir = pjoin(games_dir.name, \"\")  # So path ends with '/'.\n",
        "    # copy grammar files into tmp folder so that it works smoothly\n",
        "    assert os.path.exists(\"./textworld_data\"), \"Oh no! textworld_data folder is not there...\"\n",
        "    os.mkdir(games_dir)\n",
        "    os.mkdir(pjoin(games_dir, \"textworld_data\"))\n",
        "    copy_tree(\"textworld_data\", games_dir + \"textworld_data\")\n",
        "    if agent.run_eval:\n",
        "        assert os.path.exists(pjoin(data_path, agent.testset_path)), \"Oh no! test_set folder is not there...\"\n",
        "        os.mkdir(pjoin(games_dir, agent.testset_path))\n",
        "        copy_tree(pjoin(data_path, agent.testset_path), pjoin(games_dir, agent.testset_path))\n",
        "\n",
        "    if agent.train_data_size == -1:\n",
        "        game_queue_size = agent.batch_size * 5\n",
        "        game_queue = []\n",
        "\n",
        "    episode_no = 0\n",
        "    if agent.train_data_size == -1:\n",
        "        # endless mode\n",
        "        game_generator_queue = game_generator.game_generator_queue(path=games_dir, random_map=agent.random_map, question_type=agent.question_type, max_q_size=agent.batch_size * 2, nb_worker=8)\n",
        "    else:\n",
        "        # generate the training set\n",
        "        all_training_games = game_generator.game_generator(path=games_dir, random_map=agent.random_map, question_type=agent.question_type, train_data_size=agent.train_data_size)\n",
        "        all_training_games.sort()\n",
        "        all_env_ids = None\n",
        "    while(True):\n",
        "        if episode_no > agent.max_episode:\n",
        "            break\n",
        "        np.random.seed(episode_no)\n",
        "        if agent.train_data_size == -1:\n",
        "            # endless mode\n",
        "            for _ in range(agent.batch_size):\n",
        "                if not game_generator_queue.empty():\n",
        "                    tmp_game = game_generator_queue.get()\n",
        "                    if os.path.exists(tmp_game):\n",
        "                        game_queue.append(tmp_game)\n",
        "            if len(game_queue) == 0:\n",
        "                time.sleep(0.1)\n",
        "                continue\n",
        "            can_delete_these = []\n",
        "            if len(game_queue) > game_queue_size:\n",
        "                can_delete_these = game_queue[:-game_queue_size]\n",
        "                game_queue = game_queue[-game_queue_size:]\n",
        "            sampled_games = np.random.choice(game_queue, agent.batch_size).tolist()\n",
        "            env_ids = [register_game(gamefile, request_infos=request_infos) for gamefile in sampled_games]\n",
        "        else:\n",
        "            if all_env_ids is None:\n",
        "                all_env_ids = [register_game(gamefile, request_infos=request_infos) for gamefile in all_training_games]\n",
        "            env_ids = np.random.choice(all_env_ids, agent.batch_size).tolist()\n",
        "\n",
        "        if len(env_ids) != agent.batch_size:  # either less than or greater than\n",
        "            env_ids = np.random.choice(env_ids, agent.batch_size).tolist()\n",
        "        env_id = make_batch2(env_ids, parallel=True)\n",
        "        env = gym.make(env_id)\n",
        "        env.seed(episode_no)\n",
        "\n",
        "        obs, infos = env.reset()\n",
        "        batch_size = len(obs)\n",
        "        # generate question-answer pairs here\n",
        "        questions, answers, reward_helper_info = game_generator.generate_qa_pairs(infos, question_type=agent.question_type, seed=episode_no)\n",
        "        print(\"====================================================================================\", episode_no)\n",
        "        print(questions[0], answers[0])\n",
        "\n",
        "        agent.train()\n",
        "        agent.init(obs, infos)\n",
        "\n",
        "        commands, last_facts, init_facts = [], [], []\n",
        "        commands_per_step, game_facts_cache = [], []\n",
        "        for i in range(batch_size):\n",
        "            commands.append(\"restart\")\n",
        "            last_facts.append(None)\n",
        "            init_facts.append(None)\n",
        "            game_facts_cache.append([])\n",
        "            commands_per_step.append([\"restart\"])\n",
        "\n",
        "        observation_strings, possible_words = agent.get_game_info_at_certain_step(obs, infos)\n",
        "        observation_strings = [a + \" <|> \" + item for a, item in zip(commands, observation_strings)]\n",
        "        input_quest, input_quest_char, _ = agent.get_agent_inputs(questions)\n",
        "\n",
        "        transition_cache = []\n",
        "        print_cmds = []\n",
        "        counting_rewards_np = []\n",
        "        valid_command_rewards_np = []\n",
        "\n",
        "        act_randomly = False if agent.noisy_net else episode_no < agent.learn_start_from_this_episode\n",
        "        # push init state into counting reward dict\n",
        "        state_strings = agent.get_state_strings(infos)\n",
        "        _ = agent.get_binarized_count(state_strings, update=True)\n",
        "        for step_no in range(agent.max_nb_steps_per_episode):\n",
        "            # update answerer input\n",
        "            for i in range(batch_size):\n",
        "                if agent.not_finished_yet[i] == 1:\n",
        "                    agent.naozi.push_one(i, copy.copy(observation_strings[i]))\n",
        "                if agent.prev_step_is_still_interacting[i] == 1:\n",
        "                    new_facts = process_facts(last_facts[i], infos[\"game\"][i], infos[\"facts\"][i], infos[\"last_action\"][i], commands[i])\n",
        "                    game_facts_cache[i].append(new_facts)  # info used in reward computing of existence question\n",
        "                    last_facts[i] = new_facts\n",
        "                    if step_no == 0:\n",
        "                        init_facts[i] = copy.copy(new_facts)\n",
        "\n",
        "            # generate commands\n",
        "            if agent.noisy_net:\n",
        "                agent.reset_noise()  # Draw a new set of noisy weights\n",
        "\n",
        "            observation_strings_w_history = agent.naozi.get()\n",
        "            input_observation, input_observation_char, _ =  agent.get_agent_inputs(observation_strings_w_history)\n",
        "            commands, replay_info = agent.act(obs, infos, input_observation, input_observation_char, input_quest, input_quest_char, possible_words, random=act_randomly)\n",
        "            for i in range(batch_size):\n",
        "                commands_per_step[i].append(commands[i])\n",
        "\n",
        "            replay_info = [observation_strings_w_history, questions, possible_words] + replay_info\n",
        "            admissible_commands = [set(item) - set([\"look\", \"wait\", \"inventory\"]) for item in infos[\"admissible_commands\"]]\n",
        "            vc_rewards = [float(c in ac) for c, ac in zip(commands, admissible_commands)]\n",
        "            valid_command_rewards_np.append(np.array(vc_rewards))\n",
        "\n",
        "            # pass commands into env\n",
        "            obs, _, _, infos = env.step(commands)\n",
        "            # possible words no not depend on history, because one can only interact with what is currently accessible\n",
        "            observation_strings, possible_words = agent.get_game_info_at_certain_step(obs, infos)\n",
        "            observation_strings = [a + \" <|> \" + item for a, item in zip(commands, observation_strings)]\n",
        "            # counting rewards\n",
        "            state_strings = agent.get_state_strings(infos)\n",
        "            c_rewards = agent.get_binarized_count(state_strings, update=True)\n",
        "            counting_rewards_np.append(np.array(c_rewards))\n",
        "\n",
        "            if agent.noisy_net and step_in_total % agent.update_per_k_game_steps == 0:\n",
        "                agent.reset_noise()  # Draw a new set of noisy weights\n",
        "\n",
        "            if episode_no >= agent.learn_start_from_this_episode and step_in_total % agent.update_per_k_game_steps == 0:\n",
        "                interaction_loss = agent.update_interaction()\n",
        "                if interaction_loss is not None:\n",
        "                    running_avg_correct_state_loss.push(interaction_loss)\n",
        "                qa_loss = agent.update_qa()\n",
        "                if qa_loss is not None:\n",
        "                    running_avg_qa_loss.push(qa_loss)\n",
        "\n",
        "            print_cmds.append(commands[0] if agent.prev_step_is_still_interacting[0] else \"--\")\n",
        "            # force stopping\n",
        "            if step_no == agent.max_nb_steps_per_episode - 1:\n",
        "                replay_info[-1] = torch.zeros_like(replay_info[-1])\n",
        "            transition_cache.append(replay_info)\n",
        "            step_in_total += 1\n",
        "            if (step_no == agent.max_nb_steps_per_episode - 1 ) or (step_no > 0 and np.sum(generic.to_np(replay_info[-1])) == 0):\n",
        "                break\n",
        "\n",
        "        print(\" / \".join(print_cmds))\n",
        "        # The agent has exhausted all steps, now answer question.\n",
        "        answerer_input = agent.naozi.get()\n",
        "        answerer_input_observation, answerer_input_observation_char, answerer_observation_ids =  agent.get_agent_inputs(answerer_input)\n",
        "\n",
        "        chosen_word_indices = agent.answer_question_act_greedy(answerer_input_observation, answerer_input_observation_char, answerer_observation_ids, input_quest, input_quest_char)  # batch\n",
        "        chosen_word_indices_np = generic.to_np(chosen_word_indices)\n",
        "        chosen_answers = [agent.word_vocab[item] for item in chosen_word_indices_np]\n",
        "        # rewards\n",
        "        # qa reward\n",
        "        qa_reward_np = reward_helper.get_qa_reward(answers, chosen_answers)\n",
        "        # sufficient info rewards\n",
        "        masks = [item[-1] for item in transition_cache]\n",
        "        masks_np = [generic.to_np(item) for item in masks]\n",
        "        # 1 1 0 0 0 --> 1 1 0 0 0 0\n",
        "        game_finishing_mask = np.stack(masks_np + [np.zeros((batch_size,))], 0)  # game step+1 x batch size\n",
        "        # 1 1 0 0 0 0 --> 0 1 0 0 0\n",
        "        game_finishing_mask = game_finishing_mask[:-1, :] - game_finishing_mask[1:, :]  # game step x batch size\n",
        "        game_running_mask = np.stack(masks_np, 0)  # game step x batch size\n",
        "\n",
        "        if agent.question_type == \"location\":\n",
        "            # sufficient info reward: location question\n",
        "            reward_helper_info[\"observation_before_finish\"] = answerer_input\n",
        "            reward_helper_info[\"game_finishing_mask\"] = game_finishing_mask\n",
        "            sufficient_info_reward_np = reward_helper.get_sufficient_info_reward_location(reward_helper_info)\n",
        "        elif agent.question_type == \"existence\":\n",
        "            # sufficient info reward: existence question\n",
        "            reward_helper_info[\"observation_before_finish\"] = answerer_input\n",
        "            reward_helper_info[\"game_facts_per_step\"] = game_facts_cache  # facts before issuing command (we want to stop at correct state)\n",
        "            reward_helper_info[\"init_game_facts\"] = init_facts\n",
        "            reward_helper_info[\"full_facts\"] = infos[\"facts\"]\n",
        "            reward_helper_info[\"answers\"] = answers\n",
        "            reward_helper_info[\"game_finishing_mask\"] = game_finishing_mask\n",
        "            sufficient_info_reward_np = reward_helper.get_sufficient_info_reward_existence(reward_helper_info)\n",
        "        elif agent.question_type == \"attribute\":\n",
        "            # sufficient info reward: attribute question\n",
        "            reward_helper_info[\"answers\"] = answers\n",
        "            reward_helper_info[\"game_facts_per_step\"] = game_facts_cache  # facts before and after issuing commands (we want to compare the differnce)\n",
        "            reward_helper_info[\"init_game_facts\"] = init_facts\n",
        "            reward_helper_info[\"full_facts\"] = infos[\"facts\"]\n",
        "            reward_helper_info[\"commands_per_step\"] = commands_per_step  # commands before and after issuing commands (we want to compare the differnce)\n",
        "            reward_helper_info[\"game_finishing_mask\"] = game_finishing_mask\n",
        "            sufficient_info_reward_np = reward_helper.get_sufficient_info_reward_attribute(reward_helper_info)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # push qa experience into qa replay buffer\n",
        "        for b in range(batch_size):  # data points in batch\n",
        "            # if the agent is not in the correct state, do not push it into replay buffer\n",
        "            if np.sum(sufficient_info_reward_np[b]) == 0.0:\n",
        "                continue\n",
        "            agent.qa_replay_memory.push(False, qa_reward_np[b], answerer_input[b], questions[b], answers[b])\n",
        "\n",
        "        # assign sufficient info reward and counting reward to the corresponding steps\n",
        "        counting_rewards_np = np.stack(counting_rewards_np, 1)  # batch x game step\n",
        "        valid_command_rewards_np = np.stack(valid_command_rewards_np, 1)  # batch x game step\n",
        "        command_rewards_np = sufficient_info_reward_np + counting_rewards_np * game_running_mask.T * agent.revisit_counting_lambda + valid_command_rewards_np * game_running_mask.T * agent.valid_command_bonus_lambda  # batch x game step\n",
        "        command_rewards = generic.to_pt(command_rewards_np, enable_cuda=agent.use_cuda, type=\"float\")  # batch x game step\n",
        "        for i in range(command_rewards_np.shape[1]):\n",
        "            transition_cache[i].append(command_rewards[:, i])\n",
        "        print(command_rewards_np[0])\n",
        "\n",
        "        # push command generation experience into replay buffer\n",
        "        for b in range(batch_size):\n",
        "            is_prior = np.sum(command_rewards_np[b], 0) > 0.0\n",
        "            for i in range(len(transition_cache)):\n",
        "                batch_observation_strings, batch_question_strings, batch_possible_words, batch_chosen_indices, _, batch_rewards = transition_cache[i]\n",
        "                is_final = True\n",
        "                if masks_np[i][b] != 0:\n",
        "                    is_final = False\n",
        "                agent.command_generation_replay_memory.push(is_prior, batch_observation_strings[b], batch_question_strings[b], [item[b] for item in batch_possible_words], [item[b] for item in batch_chosen_indices], batch_rewards[b], is_final)\n",
        "                if masks_np[i][b] == 0.0:\n",
        "                    break\n",
        "\n",
        "        # for printing\n",
        "        r_qa = np.mean(qa_reward_np)\n",
        "        r_sufficient_info = np.mean(np.sum(sufficient_info_reward_np, -1))\n",
        "        running_avg_qa_reward.push(r_qa)\n",
        "        running_avg_sufficient_info_reward.push(r_sufficient_info)\n",
        "        print_rewards = np.mean(np.sum(command_rewards_np, -1))\n",
        "        obs_string = answerer_input[0]\n",
        "        print(obs_string)\n",
        "        # finish game\n",
        "        agent.finish_of_episode(episode_no, batch_size)\n",
        "        # close env\n",
        "        env.close()\n",
        "        if agent.train_data_size == -1:\n",
        "            # when games are generated on the fly,\n",
        "            # remove all files (including .json and .ni) that have been used\n",
        "            files_to_delete = []\n",
        "            for gamefile in can_delete_these:\n",
        "                if not gamefile.endswith(\".ulx\"):\n",
        "                    continue\n",
        "                files_to_delete.append(gamefile)\n",
        "                files_to_delete.append(gamefile.replace(\".ulx\", \".json\"))\n",
        "                files_to_delete.append(gamefile.replace(\".ulx\", \".ni\"))\n",
        "            # print(\"rm -f {}\".format(\" \".join(files_to_delete)))\n",
        "            os.system(\"rm -f {}\".format(\" \".join(files_to_delete)))\n",
        "        episode_no += batch_size\n",
        "\n",
        "        time_2 = datetime.datetime.now()\n",
        "        print(\"Episode: {:3d} | time spent: {:s} | interaction loss: {:2.3f} | qa loss: {:2.3f} | rewards: {:2.3f} | qa acc: {:2.3f}/{:2.3f} | correct state: {:2.3f}/{:2.3f}\".format(episode_no, str(time_2 - time_1).rsplit(\".\")[0], running_avg_correct_state_loss.get_avg(), running_avg_qa_loss.get_avg(), print_rewards, r_qa, running_avg_qa_reward.get_avg(), r_sufficient_info, running_avg_sufficient_info_reward.get_avg()))\n",
        "\n",
        "        if episode_no < agent.learn_start_from_this_episode:\n",
        "            continue\n",
        "        if episode_no == 0 or (episode_no % agent.save_frequency > (episode_no - batch_size) % agent.save_frequency):\n",
        "            continue\n",
        "        eval_qa_reward, eval_sufficient_info_reward = 0.0, 0.0\n",
        "        # evaluate\n",
        "        if agent.run_eval:\n",
        "            eval_qa_reward, eval_sufficient_info_reward = evaluate.evaluate(data_dir, agent)\n",
        "            # if run eval, then save model by eval accucacy\n",
        "            if eval_qa_reward + eval_sufficient_info_reward > best_sum_reward_so_far:\n",
        "                best_sum_reward_so_far = eval_qa_reward + eval_sufficient_info_reward\n",
        "                agent.save_model_to_path(output_dir + \"/\" + agent.experiment_tag + \"_model.pt\")\n",
        "        # save model\n",
        "        elif agent.save_checkpoint:\n",
        "            if running_avg_qa_reward.get_avg() + running_avg_sufficient_info_reward.get_avg() > best_sum_reward_so_far:\n",
        "                best_sum_reward_so_far = running_avg_qa_reward.get_avg() + running_avg_sufficient_info_reward.get_avg()\n",
        "                agent.save_model_to_path(output_dir + \"/\" + agent.experiment_tag + \"_model.pt\")\n",
        "\n",
        "        # plot using visdom\n",
        "        viz_avg_correct_state_acc.append(running_avg_sufficient_info_reward.get_avg())\n",
        "        viz_avg_qa_acc.append(running_avg_qa_reward.get_avg())\n",
        "        viz_eval_sufficient_info_reward.append(eval_sufficient_info_reward)\n",
        "        viz_eval_qa_reward.append(eval_qa_reward)\n",
        "        viz_x = np.arange(len(viz_avg_correct_state_acc)).tolist()\n",
        "\n",
        "        if plt_win is None:\n",
        "            plt_win = viz.line(X=viz_x, Y=viz_avg_correct_state_acc,\n",
        "                                opts=dict(title=agent.experiment_tag + \"_train\"),\n",
        "                                name=\"correct state\")\n",
        "            viz.line(X=viz_x, Y=viz_avg_qa_acc,\n",
        "                        opts=dict(title=agent.experiment_tag + \"_train\"),\n",
        "                        win=plt_win, update='append', name=\"qa\")\n",
        "        else:\n",
        "            viz.line(X=[len(viz_avg_correct_state_acc) - 1], Y=[viz_avg_correct_state_acc[-1]],\n",
        "                        opts=dict(title=agent.experiment_tag + \"_train\"),\n",
        "                        win=plt_win,\n",
        "                        update='append', name=\"correct state\")\n",
        "            viz.line(X=[len(viz_avg_qa_acc) - 1], Y=[viz_avg_qa_acc[-1]],\n",
        "                        opts=dict(title=agent.experiment_tag + \"_train\"),\n",
        "                        win=plt_win,\n",
        "                        update='append', name=\"qa\")\n",
        "\n",
        "        if eval_plt_win is None:\n",
        "            eval_plt_win = viz.line(X=viz_x, Y=viz_eval_sufficient_info_reward,\n",
        "                                    opts=dict(title=agent.experiment_tag + \"_eval\"),\n",
        "                                    name=\"correct state\")\n",
        "            viz.line(X=viz_x, Y=viz_eval_qa_reward,\n",
        "                        opts=dict(title=agent.experiment_tag + \"_eval\"),\n",
        "                        win=eval_plt_win, update='append', name=\"qa\")\n",
        "        else:\n",
        "            viz.line(X=[len(viz_eval_sufficient_info_reward) - 1], Y=[viz_eval_sufficient_info_reward[-1]],\n",
        "                        opts=dict(title=agent.experiment_tag + \"_eval\"),\n",
        "                        win=eval_plt_win,\n",
        "                        update='append', name=\"correct state\")\n",
        "            viz.line(X=[len(viz_eval_qa_reward) - 1], Y=[viz_eval_qa_reward[-1]],\n",
        "                        opts=dict(title=agent.experiment_tag + \"_eval\"),\n",
        "                        win=eval_plt_win,\n",
        "                        update='append', name=\"qa\")\n",
        "\n",
        "        # write accucacies down into file\n",
        "        _s = json.dumps({\"time spent\": str(time_2 - time_1).rsplit(\".\")[0],\n",
        "                         \"sufficient info\": running_avg_sufficient_info_reward.get_avg(),\n",
        "                         \"qa\": running_avg_qa_reward.get_avg(),\n",
        "                         \"eval sufficient info\": eval_sufficient_info_reward,\n",
        "                         \"eval qa\": eval_qa_reward})\n",
        "        with open(output_dir + \"/\" + json_file_name + '.json', 'a+') as outfile:\n",
        "            outfile.write(_s + '\\n')\n",
        "            outfile.flush()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"Train an agent.\")\n",
        "    parser.add_argument(\"data_path\",\n",
        "                        default=\"./\",\n",
        "                        help=\"where the data (games) are.\")\n",
        "    args = parser.parse_args()\n",
        "    train(args.data_path)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}